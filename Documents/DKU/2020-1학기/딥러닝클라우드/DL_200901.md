# 목차

[200901](#200901)

[200911](#200911)

[200915](#200915)

[200922](#200922)

[200929](#200929)

[201006](#201006)

[201013](#201013)

# 200901

## 수업 개요

### 교과목 목표

- 딥러닝의 기본 개념
- 딥러닝을 현실 문제에 적용할 수 있는 방법론 학습
- 카페(Caffe), 텐서플로우(Tensorflow) 등 주요 딥러닝 프레임워크
- 클라우드 환경에서의 데이터 처리

### 기대되는 학습성과

- 파이썬의 딥러닝 관련 라이브러리 활용 가능
- 머신러닝의 개념 이해(딥러닝보다 넓은 차원의 개념)
- 클라우드 컴퓨팅의 종류와 동작 방식
- Keras(프레임워크)를 이용해 딥러닝 문제를 해결

1주차

## Deep Learning

인공지능: 기계 등이 인간과 같은 지능, 판단 할 수 있게 하는 -> 이것의 연장선상에 딥러닝이 있음

![image](https://user-images.githubusercontent.com/33214449/92207133-b28f7180-eec3-11ea-9a91-68d3b779d290.png)

출처: https://www.datacatchup.com/wp-content/uploads/2019/05/image.png

## Cloud

온프레미스
IaaS
PaaS
SaaS

![image](https://user-images.githubusercontent.com/33214449/92207542-5aa53a80-eec4-11ea-97d9-d8a2d433cffd.png)

## 실습 환경

PC - Anaconda 를 이용해 Python 사용할 것
Cloud - GoogleColab

# 200911

2주차

## 주의

문법 위주의 설명. 강의에서 코드실행은 하지 않으니 코드를 실행해볼것

파이썬은 인터프리터 언어(컴파일 과정이 없음. 라인바이라인 실행. 명령어를 치면 바로 결과를 확인할 수 있는)

텐서플로같은 잘 알려진 딥러닝 라이브러리를(API를) 파이썬이 지원함
딥러닝, 머신러닝할 때 데이터를 다루는 것이 중요한데 파이썬은 numpy같은 라이브러리를 제공하고 다루기가 용이

5페이지

제곱은 \*\*

7페이지

파이썬은 명시적인 변수 선언이 없다

초기화를해야 변수를 쓸 수 있음

변수의 값을 확인하는 방법

- 1. 명시적으로 print
- 2. 변수명만 입력해도 변수의 값이 출력됨

2번째 강의 ~

8페이지

따로 char 없음
bool형은 대소문자 지켜서 쓰기 (True, False)

12페이지

`{0}` 여기에 어떤 값이 출력되는데 뒤에서 보면 x가 넣어지는 것

13페이지

`{0}` `{1}` `{2}` 안에 숫자는 인덱스를 나타내는 것

14페이지

뒤로가면 리스트보다 편한 거 쓰지만 알아야되긴하니까 함

17페이지

range()
뒤는 +1임 기억
range(5,10) 5~9까지 만듬
range로 만든 요소값은 변경 불가

18페이지

range 형을 list로 바꾸면 요소를 수정 가능

시작 숫자 생략하면 0부터 시작되는 수열
range(10) : 0~9

19페이지

tuple -> 잘 안씀 그냥 넘ㅇ어가~

a = (1) 이렇게 쓰면 int로 인식

3번째 강의~

22페이지

같은 깊이로 들여쓰기 할 것!

23페이지

and or

24페이지

enumerate 열거형
순서는 i, 실제 값이 들어오는건 n

25페이지

numpy 넘파이를이용한 배열
파이썬 라이브러리-> 모듈이라고 부름
리스트도 있지만... 넘파이는 배열을 쉽게 다룰 수 있게 해줌
리스트보다 처리 속도가 50배정도 빠르다
n차원의 배열을 선언할 수 있다.(넘파이에서 배열객체를 ndarray라고 함)

26페이지

넘파이에서 1차원 배열은 vector라고함

28페이지

numpy.arange

29페이지

벡터연산(리스트와 차이)
배열과 배열의 덧셈이 됨(c언어라생각하면 for문 돌려서 해야됐을것)
연산하려면 차원이 같아야됨

30페이지

2차원->array

array 의 크기
2행 3열 의미
dtype 데이터 타입!

4번째 강의~

31페이지

2행 10열의 배열이 생성됨

32페이지

rand 0에서 1사이 랜덤한 거 나옴

(seed 같으면 랜덤하긴해도 내가하든 남이하든 같은 랜덤값
seed 다르면 매번 다른 결과)

33페이지

reshape 원소의 총 개수는 같아야 사용 가능
벡터를 차원으로 바꿀 수도 있음, 2차원도 옆으로 늘려 벡터로 만들 수도 있고

36페이지

배열값 각각에 대해서 수학 연산 할 수 있음

37페이지

행렬곱셈 의 의미(딥러닝에서 많이 쓴대)
v.dot(w)

w.dot(v) 와 결과 다름

38페이지
잘라내 여러개 추출할 수 있다

[:5] 처음부터 5인덱스 바로 전까지 자름
[2:5] 인덱스 2 부터 인덱스 5전까지. 즉 인덱스 2,3,4

[n:] n부터 끝까지 다

39페이지

[:,1] 행은 처음부터끝까지, 열은 1행만

41페이지
함수에 대해서 알고싶을 때

43페이지

파이썬에서는 리턴값을 여러개 줄 수 있음

(파이썬은 블록단위로 다른문장 먼저 실행하면 그 데이터 갖다쓰는 코드 그다음에 실행해도 실행되네)

44페이지
배열의 값을 파일에 저장해놓을 수 있음(중요한 결과값이라면 이런 과정 필요!)

45페이지
많이쓰진 않음

# 200915

3주차

chap03_1,2,3 부분 피피티 다시 정독할 것

## 수업 개요

- AI scopes
- Machine learning
- Machine learning areas
- Development process of learning model
- Sciket-learn

## DC_chap03_1_compact

3페이지
머신러닝은 독립적인 분야라기보다는 많은 것과 관련되어있다(인공지능에 속하고, 패턴 인식, 통계학, 시각화 등과 관련 있음)

4페이지
머신러닝 이나 데이터마이닝 겹치는 부분 있어. 대량의 데이터가 관여된다.
데이터마이닝: 정보를 뽑아내는
머신러닝: 데이터를 토대로 예측하는

5페이지
인공지능: 컴퓨터가 사람처럼 행동하게끔 할 수 있게하는 기술
머신러닝: 통계적인 방법과 데이터를 활용해 문제를 더 잘 해결하게끔하는 인공지능 기술
딥러닝: 뉴럴 네트워크

6페이지
머신러닝이 뭘까? 머신이 기계일까 학습한다는 것은 무엇일까?
구체적으로 뭘 한다는 것일까?

7페이지

머신러닝 (실제적인 의미): 과거의 경험을 미래의 결정(예측)에 활용하는 소프트웨어를 디자인하고 연구하는 분야

=>과거를 공부해 미래를 예측하는 기술
과거 데이터로부터 숨겨진 규칙을 찾아내 일반화-> 이를 미래 예측에 활용

과거의 경험 자체를 잘 이해하는 것 유용한 정보를 뽑아내려고 하는 것=> 데이터 마이닝
여기서 규칙을 찾아 미래 예측에 활용하려 하는 것은 머신러닝

전통적 SW
규칙을 인간이 알아내어 알고리즘 형태로 SW 안에 구현한 것(이게 규칙이겠지~)
누가 알고리즘을 잘 만드느냐가 SW엔지니어의 능력이었음
(알고리즘은 규칙을 찾아가는 과정을 다 서술한 것)

머신러닝
규칙을 알아내는 방법은 인간이 제시(방법론만을 제시)
머신이 규칙을 찾는다. (전통적 SW 와의 차이)
머신이 과거데이터로부터 규칙을 알아내는 과정이 '학습'(머신입장에선 학습, 인간입장에선 머신을 '훈련'시키는 것)

인공지능 > 머신러닝 > 딥러닝 (포함관계)

머신러닝: 과거의 축적된 데이터를 '학습'하여 미래를 예측하는 기술

8페이지

머신 러닝은 기본적으로 데이터가 있어야한다.

예측모델 -> 규칙을 의미 (어떨 때 주가가 이렇더라)
이제 데이터가 있고 내일 주가를 모르면 그 데이터를 모델에 넣어 주가 예측할 수 있는 것

9페이지

머신 러닝에서 '머신' -> SW, 프로그램

10페이지

머신러닝이 사용하는 학습 자료는? 데이터
엑셀 형태의 데이터. sns에서 오가는 데이터들, 이미지, 음성파일 등

11페이지

'러닝'은 무엇인가?

설명변수:아까 본 주식에 영향을 미치는 요인들(여러개)
반응변수:요인들에 의해 결정되는 주가(하나)

잘 정돈된 데이터인 데이터셋(설명변수, 반응변수)를
학습방법에 집어넣으면... 러닝이 됨(머신입장)
학습의 아웃풋은 예측 모델이라함

12페이지
러닝의 사례들

13페이지
이런 학습 알고리즘을 잘 사용할 줄 알면됨

15페이지
관건은 얼마나 정확한 모델을 만드느냐가 관건

## DC_chap03_2_compact

17페이지

우리수업에선 지도학습 주로 학습(비지도학습은 조금, 강화학습은 다루지 않음)

- 지도학습 (가이드하는 학습방법)
  문제, 답을 같이 줘서(설명변수, 반응변수를 같이 주고) 학습시키는 것이 지도학습

  - 회귀(답의 형태가 수치형. 값이 있고 크기 비교가 가능한)
  - 분류(답이 범주형.)->수업에서 제일 많이 다룰 것

정답의 형태에 따라 회귀, 분류로 나눠짐
맞추고자하는 답의 형태가 수치형이면 회귀
(분류문제 가장 많이 다룸)

- 비지도학습
  문제만 주고 답은 안줌

18페이지

세모를 모델에 넣으면 -인지 +인지 알려주는 (분류의 예)

처음엔 그룹이 뭔지도 모름
그룹이 어떻게 구성되는지 찾아봐라(군집화)

위 둘의 차이는
분류는 그룹 정보가 이미 주어진다.
군집화는 그룹 정보 없이 스스로 그룹 정보를 만들어내야함

19페이지

오존이 120 일때 기온은 얼마일까? 예측 가능
예측해야될 대상이 클래스나 범주가 아니라 숫자라면
regression

20페이지
처음에는 비실비실한데 나중에는 잘 치게 됨

## DC_chap03_3_compact

요 영상 중요하대

22페이지

개발자 입장에서 어떻게 모델을 개발하는지 과정 도식화한 것

학습에 사용할 training data
학습에 사용하지 않는 test data 로 나눔

만든 모델의 성능을 평가하는 것이 굉장히 중요
(평가를 어떻게 하지? 어차피 미래를 예측하는 건데?)

test data가 미래 데이터로 보고
학습에 안쓴 test data를 미래 데이터로 보고 이걸 가지고 예측해 예측 결과를 도출.
그리고 test data는 이미 결과를(Y) 가지고 있으니까 예측한거랑 이거랑 비교해서 모델을 평가

validatoin data 는 현재 이루어지고 있는 학습 과정이 잘 가고 있는지를 평가하기 위한 데이터이다.(부족한 부분 보충) 어떤 모델은 이게 있고 어떤 모델은 없는 알고리즘도 있음

---

머신러닝의 아웃풋은 '학습 모델'

22페이지 표 자주 나올 것

test data(일부 남겨놓은)를 미래 데이터로 보고
그 학습과 학습시킨 모델과 비교

비교해 많이 일치할 수록 모델이 정확도가 높다 이렇게 판단할 수 있음

---

24페이지

각각 모델 의 결과와 이미 있는 결과(Y)를 이용해 accuracy 도출 가능

왜?
training data를 통해 학습했기 때문에 training accuracy 정확도가 높음

## DC_chap03_4_compact

파이썬에서 머신러닝을 위해 사용되는 두가지 라이브러리 소개할 것

25페이지

1. sciket-learn (라이브러리1)

여기 있는 함수 많이 배울 것

26페이지

2. pandas (라이브러리2)

학습에 쓸 데이터가 보통 파일에 저장되어있는데 그걸 불러와서
데이터 프레임(넘파이에 있는 것 처럼 2차원 배열같은)에 저장할 수 있음
그리고 그 데이터프레임을 가공 가능(어디부터 어디까지 잘라서 쓰고 등등)

27페이지

꽃받침의 길이와 너비, 꽃잎의 길이와 너비(x) 주고 이게 품종(y, 결과)이 뭘까? 분류할 수 있을것(이 학습데이터를 바탕으로)

28페이지

iris 가 데이터프레임을 담고있게됨

with display.max... 생략된 데이터 없이 행, 컬럼 다 보일 수 있게함

head() 데이터를 앞부터 몇개만 보고싶을 때
인수로 숫자를 주면 몇개도 설정가능

tail()은 그 반대

iris.shape 행과 열만 알고싶을 경우

columns 컬럼의 이름을 보고싶은경우(컬럼은 실데이터는 아님)

columns[:4] 앞에서 시작해 4번째까지(0,1,2,3)

iris['컬럼이름'] 해당하는 컬럼 데이터만 가져옴
iris[['컬럼이름', '컬럼이름']]
이렇게 잘라내고 또다른 변수에 저장해 데이터프레임처럼 쓸 수 있음

iloc도 기준이 행은 인덱스고 열은 0부터시작하는 값인듯
iloc 인덱스를 이용해 자르는 방법
iloc[90,4] 인덱스 90에 해당하는 행과 4에 해당하는 열 -> 열도 0부터시작하니 실제론 5번째 열에 있는 데이터가 나올 것

iloc[10:50, 0:4] 10~50 행, 0~4열에 해당하는 데이터 가져오는
iloc[10:50, :] 10-50 행, 열은 모두 라는 의미

조건을 주고 자르는 경우
논리식은 데이터마다 t, f 판별한 정보 가지고 있음
iris[setosa] 품종이 setosa 인 것만 보여줄 것
& -> and의미 두 조건을 만족하는 것만 골라오는 것

30페이지

어떤 모듈 설치되어있는지 확인 가능한 명령어

# 200922

## 수업 개요

1. Simple linear regression 단순 선형 회귀
2. Multiple linear regression 다중 선형 회귀
3. Logistic regression

회귀문제 이면서 분류도 풀 수 있음 오늘 다룰 것

## 수업 내용

### DC_chao04_1_compact

1. Simple linear regression

4페이지

독립, 종속 변수의 두 관계가 선형관계인지 파악하고(하나 증가하면 다른 하나도 증가하거나 하나 감소하면 다른 하나도 감소하거나) 이를 예측에 활용하는

모델 = (구체적으로 말하면)학습 모델

머신이 W,b 같은 상수를 알아내게 하는게 목표임

6페이지

산점도

오른쪽 두개는 선형관계가 아닌 경우임

오른쪽위는 이차식이 될 것

왼쪽것에 단순선형회귀를 적용할 수 있다.

7페이지

(a) 가 더 좋아보이는데
이걸 객관적으로 평가할 수 있는 척도가 필요함

8페이지

오차들의 합계가 가장 적어야함

예측값-실제값이 마이너스인 경우도 있고 플러스인경우도 있음
그래서 각각을 제곱해서 더해줌.

12페이지

regression 하려면 넘파이 배열로 담아줘야함.
그리고 reshape으로 2차원 배열이 되도록해줘야해(여기서는)

13페이지
test_size=0.2
test data를 20프로로 하겠다는것(그럼 training data는 80프로가 되겠지)

random_state 랜덤 시드값 (실행시마다 랜덤으로 정하는 것 고정시키려고)여기선 train test 나누는 것을 랜덤으로 햇을때 그걸 이후에도 사용하기 위해서

14페이지

모델만들 때 방법 지정

fit=learning 학습하여 W,b얻을 수 있음

predict로 예측
pred_y랑 test_y랑 비교해서 정확한지 보면되겠지

15페이지

값 하나로 예측하고 싶을 때(곽갈호 2개인 것에 유의-predict가 2차원 배열을 받아들이기 때문에)

16페이지

실제는 아래 코드처럼 써야함

W,b 출력할 수 있음(학습이 끝난 모델안에 들어있는 것)

17페이지

모델 평가 1

mean squared error
실제값-정답값 의 제곱을 더해 평균낸 것(오차의 제곱의 합 평균)
이 값이 작을 수록 정확한 모델

0이면 예측이 정확하다는 것인데 이는 이상적인 값

근데 이 오차가 얼마나 작은것인지를 추측하기가 힘듦

18페이지

모델 평가 2

r2 score
1에 가까울 수록 좋은 모델인 걸 알 수 있기 때문에 모델 평가 1보다 보기 쉬움

19페이지

scatter 산점도 그리는 것
plot 라인 그리는 것

### DC_chao04_2_compact

2. Multiple linear regression
   중 선형 회귀
   다중 선형 회귀

20페이지

독립변수가 2개 이상인 경우(단일 선형 회귀와의 차이점)

intercept 편차

22페이지

c income 연봉을 예측하고싶음(선형)

교육년수, 여성비율, 직업에 대한 평판 쓸 것

26페이지

단순 선형 회귀와 코드 방식은 비슷

27페이지

독립변수가 3개라 coefficient계수도 3개가 나옴

28페이지
각괄호 2개 해도 되고
이렇게 reshape 써도 되고

### DC_chao04_3_compact

3. Logisic regression

분류 문제를 회귀 방법으로 풀려고 하는 것

30페이지

이것도 어차피 회귀라 품종도 숫자로 나타내야함

33페이지

전체 정답중에 정답을 맞춘 개수

분류가 logistic regression 보다 성능 좋지만 배우는 건 필요하니까!

# 200929

5주차

## DC_chap05_1_compact

1. remind: clustering, classification

3페이지

clustering(군집화)
비슷한 데이터들끼리 묶는 작업
(알아서 묶음)비지도적 학습
거리가 가까운 것들끼리 묶음-> 거리계산이 중요

4페이지

classification 분류
이미 범주를 알고 있음
남자 / 여자 나눌 때 어떤 정보가 남자인지 여자인지 구별
예측, 진료에 많이 쓰임
이 수업에서 많이 배울 것
지도적 학습(이미 범주데이터-정답이 주어지기 때문에)

6페이지

이렇게 그룹을 만들어 해석.
속도는 느리고, 무게는 무겁고...~~~ 아마 화물차일것

그룹지어짐>특성 존재

7페이지

환자와 정상인
아래 데이터를 보고 어떤 범주(환자or정상)에 속하는지 구하는

8페이지
분류는 이미지를 가지고도 할 수 있다.

사진속 장소 이름 뭔지 모르겠을 때 > 비슷한거 찾음

9페이지

분류가 두개일 때 -> binary

multiple
알파벳은 스물 몇개겠지(클래스가)

binary vs multiple 뭐가 쉬울까?
binary가 쉽겠지. 찍어도 50프로니
모델의 정확도가 비교적 높다는 뜻

## DC_chap05_2_compact

2. k-means clustering

11페이지

서로 다른 주파수대역에서 음파를 측정
금이가고 안간 타일을 구분하기 위해

군집화해서 금간것끼리 금안간것끼리 묶여야 사용 가능

12페이지

로그값을 취해 값을 좀 줄여줌(logarithms)

k=>클러스터의 개수
금이간 타일 정상 타일 예는 k=2이겠지

14페이지
가상의 점 k개 찍기
-> 각 클러스터의 중심점이 될것
이 점들과 나머지 원래 데이터들의 각각 거리 측정
(각각의 점이 가상의 점들 중 뭐랑 가까운지 계산하고 그룹화)
더 가까운 점을 포함시켜 중심점 계산(x좌표 다 더하고-가상 점 포함- 점 개수만큼 나눔.)

![image](https://user-images.githubusercontent.com/33214449/95008868-68cbaf80-0658-11eb-80a2-c8945025af8c.png)

15페이지

14페이지 반복

22페이지
labels
각 데이터의 클러스터 뭔지

붙여서 잘 보여주려고 한것
hstack 수평으로 쌓기

reshape(-1,1) 행은 너가 알아서 맞춰 라는 뜻
21페이지에 있는 데이터랑 붙이려면 reshape 해줘야해(세로로 세우는 작업)

cluster_centers 중심점
첫번째께 0번 클러스터, 두번째께 1번 클러스터

[0,0]은 어떤 클러스터일까 이렇게 추측해보는 것 (predict)

## DC_chap05_3_compact

3. KNN classifier

25페이지

별 데이터는 동그라미인지 세모인지 모른다고 할 때

knn
모르는 데이터에 가까운애들을 줄임(7-nn)
그중 많은 클래스를따른다.(여기선 동그라미)

26페이지

클래스수가 동수일때는 어떻게하나?
K를 몇으로 해야되나 이슈가 발생

27페이지

거리계산은 k-means에서 봤던 식과 같음

28페이지

루트 n보다 작게 k를 잡아라

29페이지

색깔있는 숫자가 정답

아래 색깔 그려진건 예측값

k숫자가 커질 수록 예측값 정확도가 점점 떨어짐을 볼 수 있음

k 1,3,5 다 해보고 제일 정확한걸 고름
하이퍼파라미터: 모델을 만들 때 모델의 정확도(성능)를 결정하는 몇가지의 변수들을 의미(knn에서는 하이퍼 파라미터가 k 하나겠지)

30페이지

데이터 분포가 어떻든지 분석 가능(통계적 가정 불필요)

데이터 커질 수록
새로운 데이터를 데이터들과 거리를 계산해야되서
데이터를 메모리에 다갖고있어야됨

계산도 많이해야되니까 처리시간 증가

32페이지

k값을 3으로 줌

fit함수 -> 학습하는 것

classfication 분류에서 모델 성능 평가시 많이 쓰는 것 accuracy
(전체 테스트 데이터 갯수중에 정답맞힌 갯수)

34페이지

ex예로 거리를 계산해보면
실제로 차이가 많이나는 건 시력인데(1.0 과 0.1)
거리 값에 영향을 크게 미치는 건 키 값임을 알 수 있음
둘의 스케일이 다름

키도 0~1로 시력도 0~1로 동일한 영향력을 갖도록 스케일링을 해줘야함

kmeans knn 하려면 스케일링 해야함

## DC_chap05_4_compact

4. perfomance metric

37페이지

성능 평가!

binary classification의 경우 이런 성능평가 척도 가지고 있다.(why 모델이 어디 적용되느냐에따라 다양한 것이 사용됨)

38페이지

T : 잘 맞춘것

FP,FN은 오류
FN이 더 심각하지(코로나라고 생각하면)

accuracy는 경우중에 정답인 것만

39페이지

민감도
ex) 양성인 사람중에 양성이라고 예측한 비율

특이도
ex) 음성인 사람중에 음성이라고 예측한 비율

의료환경에서는 무엇이 중요할까? 민감도가 중요할 것.(환자인데 음성이라고 진단한 경우가 높으면 큰일이니)

40페이지

정밀도

양성이라고 예측한 사람들 중에 진짜 양성인 사람

41페이지

f1 score
민감도와 특이도의 불균형 측정

민감도가 100퍼센트인데(1) 특이도가 0이면 f1 score는 0

불균형한 경우 척도값이 낮게 나오는게 f1 score

42페이지
class가 3개이면 성능 측정 어떻게 하나?
(multiple class model)

클래스별로 민감도 특이도를 계산할 수 있으나 잘 쓰지 않음

43페이지

classification에서 쓰는 척도들

모델을 만드는 것도 중요하지만 성능을 측정하는 것도 중요

45페이지
인자 줄 때 정답이 앞에 예측이 뒤에 와야됨!

46페이지
3x3행렬보면...
test_y가 정답임

정답은 0 인데 예측도 0으로한건 2개라는 뜻
대각선이 제대로 맞춘 것들을 나타냄

## DC_chap05_5_compact

5. k-fold cross validation

validation이니까 평가해서 알아보는 것임

이 결과를 믿어야할까?라는 것
training , test 데이터를 어떻게 나누느냐에 따라 정확도가 엄청 달라질 수도 있음

-> 해결하려면
나누는 걸 여러번 해서 평균을 할 수도 있음
아니면
k-fold cross validation사용

50페이지

k=4면 전체를 4등분하는 것

ex1에서 진한걸 test, 나머지 training
ex2도
ex3도
ex4도

각각의 accuracy를 평균하고
이걸 그 모델의 accuracy로 보자는 것

k=10이면
10번 나누겠지
10번 모델을 만들거고

51페이지

랜덤하게 나누니까 random_state 줘야함(이후에도 같은 결과 얻으려면)
shuffle 데이터를 마구 섞고 등분할거냐 물어보는 것

knn사용한 것

accuracy 저장할 크기 5인 배열 선언

53페이지

np.mean 평균

54페이지

좀더 단순한 방법

fold쓰지 않고 cross \_val_score 사용

단지 accuracy만 구하기 때문에 사이에 코드를 넣으려면 전 페이지처럼 자세하게 하는게 좋을 수도

55페이지

k-fold
원하는 모델을 도출하지는 않음(모델 여러개 나오잖아)
정확도를 추정하는 용도

knn쓴다했을 때
k정해야함. 여러번해서 좋은거 써야함
이때 k-fold 써서 k가 몇일때 정확도 몇이고 이런걸 구하는 것(하이퍼 파라미터 값 확정에 이용됨)

하이퍼 파라미터 튜닝 아직 안배움

feature selection
컬럼 중 모델에 도움이되는것 골라내는 과정
->에도 k-fold 쓴다.

오늘 중요한 내용들 많이 배웠대
다양한 데이터셋으로 연습할 것

# 201006

6주차

classification(분류) 방법 중 하나인 의사결정나무(decision tree) 학습할 것

## DC_chap06_1_compact

3페이지

분류에서 학습함이란,
클래스를 나누는 경계를 얼마나 잘 찾느냐(직선, 곡선 얘기)
3차원이 되면 면이 될 수도 있기 때문에 hyper plane이라고도 함.(클래스와 클래스의 경계)

4페이지

분류를 할 수 있는 많은 방법이 있음

svm
decision tree
random forest
주로 배울 것

xgboost 요즘 많이 쓰는. 수업에선 소개만 할 것

머신러닝처럼 노가다도 이런 노가다가 없다 그렇게 느끼게 될 것임

## 1.Decision Tree

5페이지

x1, x2값을 주고 색깔을 예측하는 결정 트리
sibsp 가족수

9페이지

decision tree 가 경계를 찾는 과정 배울 것

step1
일단 먼저 특정 두 클래스를 구분하는 선을 찾음

11페이지

전 페이지에서 보면 일단 split1 기준

(차지하는 수 / 전체 경우의 수)

celebrities(유명인사, 탑급배우) 나누는건 앞페이지에서 처음에 나눈 split1 선 기준으로 생각하면 됨(y축)

budget(예산)

12페이지

어떤 속성부터 선택할 것인가가 문제가 되는데...
고혈압 vs 정상 일때 두 클래스의 겹쳐지는 부분(교집합)이 제일 작은 것을 루투노드로 사용한다.(여기선 몸무게겠지)

13페이지

split을 언제 중단할 것인가(몇개까지 나누느냐)
경계선을 많이 나누면 예측은 정확해지겠지만, overfitting 발생(과적합) test데이터로 학습을 하면 예측이 떨어질 수도 있음. 적당할 때 트리 생성을 중단할 수 있어야함(과적합은 다음 시간에 배울 것)

14페이지

명목속성(범주형 자료)
속성(컬럼)

단점
훈련 데이터가 조금만 바껴도 트리가 크게 달라질 수 있음

대각선은 만들 수 없대 이게 단점!

## DC_chap06_2_compact

19페이지

category 클래스 정보
drinks 알코올

간 질환 있는지 검사하는 것이 목적

20페이지

df_X 코드부분 -> category가 아닌 것들만

22페이지

decisiontree 만들 때 랜덤하게 선택하는게 들어가기 때문에 random_state 값 줘야함(모델 정의할 때. 지정하지 않은 값은 디폴트값이 들어감)

train accuracy 1.0나왔는데 이건 100퍼 라는 뜻

confusion_matrix
23, 14가 에러고 나머지 대각선은 뭐라는데 모르겠음

23페이지

모델 정의시 인자 더 줄수도 있음
트리 깊이를 4로 제한.(자식이 생길 때마다 깊이가 1씩 늘어남)

트리깊이 적용했더니
test accuracy는 성능이 더 좋아졌음.

15,17 -> 32개의 오류 발생.(22페이지보다 좋아졌음)

-> 매개변수를 잘 조절하면 모델의 성능이 좋아질 수 있다.(다른 매개변수도 조정해 최상의 결과 내는 게 목표)

26페이지

하이퍼 파라미터
:모델 성능에 영향을 미치는 매개변수

min_samples_split
0~1 사용해 퍼센트로 줄 수도 있음
가지 분리시 각각 분배될 수 있는 최소 샘플수(10개를 두가지로 분리시 분리되고 나서 있을 수 있는 최소 샘플수)

27페이지

min_samples_leaf
하나의 노드가 가지고 있어야할 최소 샘플 수

max_features
컬럼을 모두 쓰진 않는다고 했지. 컬럼중 몇개로 제한할 것이냐

class_weight
클래스의 중요도
환자가 환자로 판명되는게 (반대보다) 더 중요하니까 (그런말임) 안그러면 큰일나자녀

⭐ 모델 만들 때 다양한 매개변수 줄 수 있음

중요한 것

- 어떤 모델을 쓸 것이냐
- 방법론이 결정됐을 때, 매개변수를 어떤 걸 지정해서 모델을 만들 것이냐

여러가지를 시도해봐야 어느방법, 어떤 방식이 좋은지 알 수 있음.

## DC_chap06_3_compact

## 3.Random Forest

29페이지

결정 트리를 응용한 개념

트리를 하나가 아니고 여러 트리를 만들어보자는 것

그럼 예측은 어떻게 하느냐
정답을 모르는 값을 트리1에 넣어보고
트리2에도 넣어보고
트리3에도 넣어보는
결과중 가장 많은(다수결)결과로 결정해서 예측하는 것

트리를 만들 때 랜덤하게 만들기 때문에 random forest

bagging
모델을 여러개(n개)를 만들어서 각각의 모델의 결과들을 취합해서 결과를 내는.

random forest는 bagging을 구체적으로 구현한 한 방법이다 라고 볼 수 있음

30페이지

bagging
데이터 인스턴스도 샘플링하고
컬럼도 샘플링한대

33페이지

n_estimators 트리의 개수 몇개로 할 건지

34페이지

트리 개수 올리니 성능 더 좋아졌음

매개변수의 값을 바꿔가면서 모델 성능을 향상시키는 방법을 하이퍼파라미터 튜닝, 모델 튜닝이라고 함.

35페이지

중요한 파라미터들을 적용하는 것이 필요
(파라미터 많아지면 튜닝 시간 길어지니)

## DC_chap06_4_compact

## 4.Support Vector Machine

37페이지

두 아이디어가 사용 1)경계선을 찾을 때 (모든 점의 정보를 이용하지 않고 경계면에 있는 몇개의 점을 이용해서 찾는다)클래스와 클래스 사이에 몇개의 점을 사용해 경계를 찾는 것. 그 몇개의 점을 support vector라고 함
-> 클래스 주어졌을 때 이 서포트 벡터를 어떻게 찾아내느냐가 관건

2. 2차원을 3차원으로 변형하게 되면 더 쉽게 나눌 수도 있음. 차원을 하나 높여 경계(면)을 찾아보자. (차원을 하나 높이는 방법이 kernel이라고 함)

38페이지 이론적 내용이라 스킵

39페이지

svm.SVC 분류(classification)에 많이 쓰임

40페이지

libsvm을 파이썬으로 재구현한 것
학습시간이 데이터셋의 샘플 수의 제곱수준으로 학습시간이 길다. (인스턴스가 많아지면 학습시간이 지수적으로 증가. 특정 개수 만개 넘어가면 실용적이지 않대 오래걸려서. 교수님은 만개정돈 괜찮대)

너무 오래걸리면 linearSVC이런거 고려해봐라는 지침

43페이지

매개변수
kernel 데이터셋의 차원을 높이는

모델 성능 떨어졌네

44페이지

c, kernel 주목!

C:
과적합이 일어나지 않도록 조절하는데 쓰이는 매개변수

kernel:
데이터셋의 차원 변형하는

degree는 poly 쓸 때 의미가 있음
다항식의 항의수?..

gamma
커널이 rbf, poly, sigmoid 일 때 의미가 있음
커널의 coefficient값을 정해주는

45페이지

여기만 봐서는 뭐가 좋은지 잘 모를 수도 있음
accuracy 확인 필요

1보단 2가 나은듯
3은 파란부분 잘 분류됐고 괜찮은듯
4는 이런식
1,2 보다는 3,4가 좋아보이는데 정확한건 accuracy 확인 필요 -> 실습통해 확인하기

46페이지

오랫동안 사랑받은 모델링

모델이 왜 이런식으로 결과를 내는지 해석이 어렵다.

## DC_chap06_5_compact

## 5.xgboost

48페이지

앙상블(여러개 모델을 만들어서 이를 통해 단일한 결과를 내도록하는... 예측 성능을 높이는 방법론)
그 중에 bagging, boosting 이 있음

bagging
모델을 여러개 만들어서 투표해서 결과를 내는.

boosting
예측 잘 안되는 그룹을 가지고 잘 될 수 있도록..
잘 안되는거에 중점을 둬서 이걸 예측을 잘해내면 점수를 더 주는
여전히 안되면 그걸가지고 또 모델을 만들고
... 그러면 모델이 업그레이드 된다
여긴 투표라는게 없음

49페이지

예측력이 평균적으로 최강이래 xgboost가!
복잡하긴 함
사이킷런 안에는 포함되어있지 않은 상황

50페이지

혼자 공부하려면 이런식으로 설치해서 써봐

## 6번 과제

Q1에서

scaling 작업을 해야 kfold를 할 수 있는 걸 알게됐음. scaling 복습 필요

# 201013

- 수업 내용

## DC chap07 01 compact

## 1. Bias-Variance trade off

3페이지

편향 분산 trade off
머신러닝 교재에서 중요한 부분으로 항상 다루는

예측모델의 에러와 관련이 있음

3가지 요인에 의해 모델의 예측 에러가 발생한다
3가지:
noise(제거 불가능한 에러 ex.이미 데이터에 잘못 기록한 경우. 환자가 아닌데 환자라고 기재했다던가. 원래 데이터 자체가 오류 담고 있는 경우)
bias(은행에서 돈 빌려줄 때 갚을 수 있을것인가 아닌가 예측하는데 예측으로는 갚을 수 있는데 갑자기 도산해서 못갚는 경우.. 이런 특이한 사례도 있음)
variance(분산)

대처할 수 있는 건 bias, variance

bias
ex.전체 중에 일부만 열심히 학습하는
전체 정보를 골고루 학습하지 못해서 에러를 증가시키는 현상
과소적합 유발

variance
데이터의 너무 세세한 부분까지 학습해 새로운 데이터 추가시 모델 변동성이 커지는
ex. 한 사람한테 너무 세세하게 맞는 옷 학습. 조금만 다른 사람와도 옷 안맞는걸로 예측
과적합 유발(저번에 한번 언급한적있지!)

4페이지

bias와 variance를 동시에 줄이는 모델을 만들 수는 없다.(trade off)

bias가 높은 ex)옷이 너무 딱맞아
variance가 높은 ex) 옷이 그냥 자루같은

오버피팅: 모델이 너무 복잡할 때
언더피팅: 모델이 너무 간단할 때

## DC chap07 02 compact

## 2. Hyper Parameter tuning

8페이지

하이퍼 파라미터 튜닝하면 좋은 모델이 나오지만(성능 좋은)
고된 작업이 되기도 한다.
좋은 모델을 찾는 것은 그 모델에 적절한 하이퍼 파라미터를 찾는 작업이다.

파라미터에 대한 조합으로 모델 만들때
k-fold cross validation 많이 사용

9페이지

하이퍼파라미터튜닝 = 최적화

random forest를 예로 한 것

10페이지

하이퍼 파라미터 튜닝을 위해 제공하는 것들

randomized : grid search를 변형한 방법?

12페이지

13페이지

pprint 프린트를 이쁘게

폭은 80, 들여쓰기 4

16페이지

grid search cv 사용해보자!
6가지 파라미터에 대해서만 튜닝을 해보겠다는 것

n_estimators 트리 몇개할건지(random forest 방법)

20페이지

2880개의 조합을 학습한 것이다 라는 것
5 fold 이고...

best*params* 베스트 조합!

21페이지

정확도 낮은 이유는
파라미터 경우의수가 작기 때문. 이걸 늘리고 하루? 오래 결과 기다리면 정확도 더 올라갈 수 있을 것

22페이지

random search cross validation

gridsearchcv는 모든 조합인데
randomizedsearchcv는 랜덤하게 골라서 하는 것(장점: 시간이 줄어듦)

25페이지

그리드와 차이점

24페이지

이렇게 변수를 지정하여 파라미터쪽에 넣어줄 수도 있음

200부터 2000까지를 10개로 나누어서~

linspace 10부터 110까지 11개로 나눔

이렇게 넣어서 조합의 일부만 랜덤으로 꼽아 사용

26페이지

n_iter 조합에서 몇개를 고를 건지
random state 조합에서 랜덤하게 뽑으니까 이를 고정하기 위해

27페이지

500개중 100개만 고름

28페이지

1프로 정도 성능 향상됐음 알 수 있음

⭐연습해보기

## DC chap07 03 compact

## 3. Model comparison

모델 비교

30페이지

모든 데이터셋에 대해 좋은 성능을 보이는 짱짱 알고리즘은 없다.
다양한 알고리즘을 써서 테스트를 해야함(가능한 많이)
그럼 쉽게 테스트할 수 있는 방법은 없을까?->사이킷런에서 제공

31페이지

labelencoder 문자열-> 숫자
당뇨. 레이블 데이터 가 지금 문자열이니까 숫자로 바꿔주려고 쓰는 것

32페이지

logistic regression은 숫자로 바꿔줘야하기때문에 encoder 사용(다른 알고리즘들은 괜찮지만)

33페이지

append 차곡차곡 넣음

인자는 두개
약칭, 모델

34페이지

result 모델 테스트할 때 나온 accuracy 결과 저장하기 위해

scoring 평가방법은 accuracy로 하겠다

kfold로 랜덤하게 나눔

cv_results~
이 모델의 cross val score를 구해라
사용할 모델, 데이터는 이렇게 주고 , cv는 세팅한 kfold로, 성능평가할 척도는 scoring에 저장되어있다

kfold로 했으니까 accuracy가 10개가 나오겠지 그래서 이거의 평균(mean)구해주는 것

std() 표준편차

35페이지

\t 로 (탭) 줄 맞춰주고
평균값만 4째자리까지 구해서 출력하는 것

36페이지

5가지 알고리즘별 accuracy

작대기 구간은 kfold로 구해진 accuracy 10개 값 범위 의미
박스 -> 그 10 개중에 가운데 5개(전체 데이터의 50프로) accuracy가 차지하는 범위
주황선 -> 중앙값(가운데 있는 값)

중앙값을 비교해 높은게 좋은 알고리즘
작대기 폭이 의미하는 바는 accuracy 변화 -> 모델의 변동 폭이라 볼 수 있음. 가장큰건 knn. lr도 큰 편. 변동폭이 큰건 좋은게 아님. 새로운 데이터 들어오면 모델이 바뀔 가능성이 높은 것. lr이 accuracy보면 가장 좋은 것 같지만 다른 rf, svm 취하는것이 더 좋을 수 있다.

## DC chap07 04 compact

## 4.feature selection

38페이지

feature=변수=컬럼(데이터셋의)

컬럼 중 일부만 사용해서 예측할 때도 있을 것
진단할 때 모두가 유용한 정보는 아닐 수도 있음
(중요도가 다를 수도 있고)

feature 중 중요한 것들을 골라 만드는 것이 중요(성능을 높이는 결과를 가져오기도함)

feature가 너무 많은 경우도 좋지 않음

39페이지

feature를 평가하는데 많은 방법이 있대. 그중에 하나 배울것

40페이지
클래스(ex. 환자와 환자아닌사람) 사이의 경계가 clear하면 좋은 feature = 교집합 구간이 짧다는 것

교집합 구간이 좁을 수록 좋을 것

실질적으로 는 filter method 사용
변수하나하나를 평가 척도에 의해 평가하는 것
평가척도 좋은 순으로 나열해 앞에서부터 n개를 선택하여 사용
단점: feature와 feature 서로 독립적이라고 가정하고 평가를 하는건데...(feature 자체가 가지고 있는 예측력만 평가하는 것- 겹치는 구간이 얼마나 작으냐) 다른 feature랑 합쳐지면 시너지를 내는 feature가 있을 수도 있음. 이를 고려하지 못하는게 단점임

41페이지

filter method -> 평가는 한번

(여러가지 고려함. filter method의 단점을 보완)
forward selection
backward elimination

실습해볼것

- forward
  좋은 애들 순으로 하나씩 가져오는

가장 좋은애를 찾음
그 다음 나머지중에 또 평가해서(상호 협력성?도 고려함) 좋은애 가져옴

끝내는 방법은 보통 2가지
개수를 미리 정하거나
정확도가 더이상 오르지 않을 때 멈추는

- backward
  안좋은 애들을 빼나가는 방법(빼는 방법도 여러가지가 있음)

언제 스톱?
개수 정할 수도 있고
빼면서 accuracy가 증가하는데 증가하는게 멈추는 부분에서 스탑할 수도 있고

## DC chap07 05 compact

43페이지

univariate feature selection -> filter method

recursive feature elimination -> backward elimination

(forward selection은 없는데 다른 모듈 써야된대)

44페이지

전체 변수 모두 사용시 정확도 구해봄

45페이지

selectkbest
각각변수중 k개 고르는

chi2 평가 척도(변수 하나하나 평가시 어떤 방법 쓸지) (카이 제곱?)

여기 예에선
shape[1] 변수의 개수 (컬럼의 개수) -> 모든 컬럼 다 선택한다는 것
(뭐하러 전체 하냐? -> 이렇게 하면 컬럼마다 평가값 알 수 있어 우리가 자의적으로 고를 수 있음)

fit.scores로 평가값보니
(chi2는 숫자가 클 수록 좋다는 것)

-fit.scores
이렇게 마이너스 주면 내림차순으로
f_order 인덱스를 정렬한 것
컬럼을 평가가 좋은 순으로 출력함
feature를 하나씩 늘려가면서 x를 지정하고 학습

46페이지

왜 df_X.shape[1]+1일까?
shape[1]은 전체 컬럼의 개수임.
아래에서 슬라이싱할때 0~i직전까지 자르기 때문인듯

47페이지

backward 방법
n_features_to_select 몇개를 남길것이냐.(여러가지 숫자로 테스트해봐야함)

어떤 feature가 선택이 됐는지(.support)
.tolist()로 예쁘게 출력

선택된 feature 만으로 accuracy 구해보는

48페이지

step
한번에 하나씩 제거할래 아님 일정 비율로 제거할래

49페이지

forward selection 사이킷런에 없기 때문에 이렇게 mlxtend 모듈 사용해야함

custom_feature_names = df_X.columns
컬럼이름을 주는것.(이걸 안하면 컬럼 index가 나온대.숫자로. 이름으로 하면 잘 알 수 있음)

50페이지

subset
과정이 다 나옴

51페이지

실습에 사용한 파라미터 정도만 사용할 줄 알면됨

53페이지

가급적 적은 변수를 이용하여 높은 정확도를 내는 것이 좋음

RFE
이용하려면 각 피처의 중요도를 기반으로 평가함
중요도 정보를 줘야해
knn을 rfe에 집어넣으면 작동이 안됨

54페이지

feature selection을 먼저 해야함

cross validation은 세 박스 때 모두 해야해

# 과제

참고

https://injo.tistory.com/10

dataframe to csv
ndarray to csv

https://stackoverrun.com/ko/q/9610780

https://cnpnote.tistory.com/entry/PYTHON-%EC%98%88%EC%B8%A1-%EA%B2%B0%EA%B3%BC%EB%A5%BC-CSV%EB%A1%9C-%EC%A0%80%EC%9E%A5

https://stackoverflow.com/questions/6081008/dump-a-numpy-array-into-a-csv-file

pandas basic

https://iludaslab.tistory.com/45

https://hogni.tistory.com/7

# 201101

- 수업 개요

본격적으로 딥러닝 배울 것. 지금까지는 머신러닝 학습함.
인공 신경망 배울 것

- 수업 내용

## DC_chap8_1

4페이지

Perceptron
인공신경망
스스로 학습가능한 것에 가능성 높임

아무리 선을 그어도 하양과 검정을 구분할 수 없는 perceptron문제 발견(xor)
-> multi-layered 되면서 해결

본격적인 딥러닝은 2010부터 시작됐다해도 과언이아님

5페이지

물체를 보고 뭐냐인지를 맞추는 분류문제(클래스(범주)가 1000개인 분류문제라고 할 수 있겠지)

6페이지

y축: 에러율

xrce이런건 딥러닝 모델들이라고 보면됨

9페이지

각 레이어들에 노드가 있다

10페이지

레이어가 얼마나 많냐 적으냐에 따라 신경망의 종류가 나눠짐

입력층에서 바로 출력층으로 연결되는건 단층신경망

딥러닝: 심층신경망을 이용한 머신 러닝

11페이지

보통 예측값은 0~1사이의 값으로 표현함

에러가 발생한걸로 네트워크(weights라는 값을)를 업데이트함
노드와 노드를 연결하는 선들에 붙어있는 값 = weights(가중치)

신경망에서의 학습의 의미
입력값을 주면 쭉 거쳐서 아웃풋을 냄
에러가 줄어들 때까지 가중치를 업데이트하고 이를 반복함
weights값이 튜닝이되는거지.
학습하고 나면 구조와 weights값이 fix가 되겠지
예측을 원하는 값을 넣으면 예측값을 주겠지.

12페이지

딥러닝 패키지 이용하면 쉽게 문제를 해결할 수 있음

우리는 keras 주로 쓸 것

## DC_chap8_2

14페이지

사용자가 다수의 신호를 입력하면 이를 하나의 신호로 출력한다.

15페이지

생물학적 뉴런을 모방

신호(자극)
신호가 수집되어 자극이 일정치 이상 넘어가면 전달되는 -> 아웃풋

16페이지

perceptron 단순한 신경망이긴한데
복잡한 신경망의 기본이 되기 때문에
용어들 숙지할 필요있음

인풋값과 노드들이 연결되는 선에는 값이 있는데 이것이 가중치⭐

중간값 v를 네모 박스(파이 함수=활성함수⭐)에 집어넣으면 y를 도출할 수 있음

b(편향)

우리는 활성함수를 0또는 1이 나오는걸로 가정하고 할 것(학습용)
(원랜 0~1사이 어떤 값이 나옴)

17페이지

x에 붙은 T는 transposi(가로 세로를 바꾼 것)

알파벳이 진하게 나오면 이건 여러값이 모인 벡터임!

18페이지

'잘 조절하는' -> 러닝의 핵심이 되겠지. 모델에서 해줄 것

19페이지

앞에서 살펴본 용어들의 실제적인 의미를 살펴볼 것

가중치(weight value) ⭐
여기선 전류가 입력값들이 되겠지
입력값의 흐름을 조절하는 역할을 하겠지
왜? 원하는 y가 나오게 하려면 잘 조절해야 하기 때문

w1이 크면 x1(입력값)도 커질 것이고
w2이 작으면 x2도 작아질 것이고

저항의 역할: 흘러가는 전류의 양을 조절함으로써 원하는 전류의 양이 흘러가도록 함
저항을 통과하면 전류가 작아짐
저항이 크면 전류가 적게 흘러가고
저항이 약하면 전류를 많이 흘려보내기 때문에 좀더 많이 흘러감

20페이지

bias(편향)

(현실에서 잘 쓰이진 않으나 개념은 알아야지!)

세타값이 0이라고 보면

v=(뭉탱이) + b
b 절대값이 크면 뭉탱이도 커야되고
b를 낮춰주면 좀 쉽게 1이 될 수 있고 너무 크면 1이되기 어렵겠지

0(비활성화)
1(활성화)

21페이지

편향이 있는 이유
(입력값으로 부터 결과 y가 나오는데) 결과값 y가 특정 쪽으로 치우치게 나오도록 조절하는 것
실제 신경망에서 잘 쓰지는 않는다.

22페이지

활성화 함수

중간계산값(가중합) v를 0~1 사이로 변환하는 역할
여러 종류의 활성화 함수가 있겠지

## DC_chap8_3

24페이지

이렇게 다섯가지 입력값이 있을 때 손으로 계산해보기

25페이지

이젠 직접 코딩해보기

range(5) -> 0,1,2,3,4
i번째 행을 하나씩 갖고옴

26페이지

and 연산

이것을 perceptron으로 구현할 수 있을까?
(xor연산은 안됨)

and 입력값 이럴 때 이렇게 결과내는 perceptron을 만들 수 있나 보는 것

28페이지

존재하는데 여러가지 경우가 존재한대

29페이지

neural netword 이용해 분류문제에 적용 많이할 것

신경망을 어떻게 만들지 고민할 것

첫번째 예

feature가 5개이니(class label 빼고) input 노드의 개수는 5개겠지
output노드의 개수 = 클래스 수

인풋값은 보통 1보다 작은 값들
아웃풋값은 0~1사이의 값

❓
hidden layer의 개수를 몇개로 할거냐?
hidden layer 별로 노드의 개수는 몇개로 할거냐?
활성함수는 모든 레이어의 노드마다 붙어있는데 이건 어떻게 할건지
bias값은 어떻게 할건지

=> 이런 것들을 정하는게 신경망을 설계하는 것

두번째 파트

노드가 2개만 있어도 세로로보면 4가지 구분할 수 있지만 이 방법 쓰진 않는대

노드 4개 쓰고
1000 이면 (세로로 본것) 0클래스이고
0100 이면 1 클래스이고
이렇게 구분을 한대(one-hot coding)

30페이지

활성함수가 붙으면
계산되서 거쳐서 나갈 때는 값이 무조건 0~1사이라는 거지

가중치 -1~1 혹은 0~1 사이 값 줌

계속 곱셈 연산이 신경망에서 이루어지기 때문에 1보다 작은 값 선호
값이 무한히 커지는 것을 막을 수 있음

31페이지

xor 한쪽만 1이어야 1

32페이지

xor 안되는 문제 해결

layer가 여러개 있는 perceptron 사용하면 된다.

33페이지

키워드보고 이게 뭐다라고 설명할 수 있으면 이해 된거겠지

# 201104

## DC_chap9_1

3페이지

y 0~1
w -1~1 혹은 0~1
x 0~1 혹은 -1~1 로 변환하여 넣음
(곱셈연산이 많기 때문에 이렇게 1보다 작은 값 많이 사용)

4페이지

w값을 조정하는 것을 반복하여
y(예측값)과 정답 사이의 오차가 줄어듦
이게 뉴럴 네트워크의 학습 원리

-> 어떻게 조정하는가?(오늘의 학습 목표)

5페이지

편향(b)없을 때

6페이지

y = f(x)

7페이지

softmax

y로 가기전에 노드(동글뱅이)가 여러개면 소프트맥스쓰는게 좋음
(class가 3개면 이 노드도 3개래 저번시간에 설명한 것처럼)

8페이지

어떤 입력값있을 때 결과가 이렇게 나오면 얘는 class 1일 확률이 65.90프로 라고 해석할 수 있겠지

다른 값들도 사용해서 0~1값을 내는 특징이 있음

softmax와 sigmoid 모두
v가 어떤 값이 들어오더라도 0~1사이 결과를 내는 공통점이 있다.

10페이지

--실제 어떻게 학습하나--

delta rule

그림

ei = 에러(오차),차이

class가 2개인 경우

입력값중 큰게 있으면 결과에도 영향 크니까
오차에도 영향크겠지 -> 이 입력값에 연결된 w 많이 조정

11페이지

활성화 함수가 그냥 가중합 그대로 통과시키는 함수라 할 때

뒤의 w: 조정 이루어지기 전에 w(갱신전의 w)
앞의 w: 갱신 후 w

델타w = 에러 _ 입력값 _ 알파
(에러가 크면 클 수록, 입력값이 크면 클 수록 델타w는 커짐 -> 피피티 전 페이지의 내용과 통하는 얘기)

12페이지

v는 각 입력값에 가중치 곱해서 각각 더하면 됨

d가 정답

e는 에러

아래 식 ej\*xi 로 고치기

13페이지

w가 변동을 조금씩 하면 에러가 줄어드는 정도도 적겠지. 학습시간이 길어짐

15페이지(손으로 계산)
e는 에러
에러가 줄어드는지 확인해보기

## DC_chap9_2

위에서 배운 것 과 달리

활성함수가 어떤게 들어와도 처리할 수 있는 보편화된 델타룰을 배워보자
미분이 관여됨

17페이지

델타 제이! = 파이의 도함수(활성함수를 미분한 것)*가중합*에러

2번이 처음 배운 델타룰과의 큰 차이점

18페이지

그대로 통과하는 친구의 경우
(우리가 처음 배운 델타룰)

19페이지

함수가 시그모이드라면?
(수학수업아니니까 자세한 계산과정은 생략)

20페이지

파이(v)는 y이기 때문에
시그모이드에선 이렇게 간단하게 표현될 수 있음

21페이지

왜 미분해야 에러가 줄어들까?

경사가 작은쪽으로 한발한발 내딛으며 내려가겠지(그럼 목표지점인 평지에 다다를 수 있음)

22페이지

사람이 w, 옮겨가는게 w
미분을 하는 이유는 w를 증가시켜야하는지 감소시켜야하는지 알 수 있기 때문(그래프 왼쪽쯤에 있으면 w를 증가시켜야할거고 오른쪽쯤에 있으면 감소시켜야할것)

조금씩 내려갈지 성큼성큼 갈지를 알파가 결정함

폭이 크면 에러가 빠르게 줄어드나 정답부근에서 왓다갓다하거나 혹은 진동해서 에러값이 주나 싶더니 늘어나는 경우도 있음
폭이 작으면 w가 조금씩 변화하니까 에러가 조금씩 줄어 학습시간이 오래걸림. 대신 정답에 근접할 가능성이 커짐

## DC_chap9_3

지금까진 일반화된 델타 롤을 살펴봤음

28페이지

w는 어떻게 표현하나?

29페이지

2가지 방법이 있음
3\*2가 좋음(x1이 첫줄, x2가 두번째줄,... 잘맞음)

w12(인풋이 1이고 (동글뱅이)아웃풋이 2)

31페이지

wt(행과 열을 바꾸는) -> 곱해주려고 트랜스포즈함

y는 파이에 v를 넣은 결과

32페이지

파이썬 행렬 연산은 matmul 사용

33페이지

행렬 연산 많음
(아직 안배운 히든 레이어 까지 들어가면 연산 엄청 많음)
연산 gpu로 해결
행렬 연산을 병렬로 처리해 학습시간 줄이는
또는 분산병렬처리(어렵대)

딥러닝 돌리려면 컴터 스펙 좀 되어야해

34페이지

원핫코딩

클래스개수에 따라 아웃풋 노드 개수 맞추고
각 노드에서 나온 값을 모아 클래스를 식별

target 클래스

keras 쓰면 더 단순하게 할 수 있음

## DC_chap9_4

36페이지

품종 맞추는

총 5개의 컬럼중 하나가 class label
변수값은 4개 -> 인풋노드 4
클래스가 3개니 -> 아웃풋 노드 3

w는 -0.5 ~ 0.5 사이 값으로 랜덤하게
runif() 사용

1000번 w 업데이트

38페이지

둘 중에 뭘 전치해야하나
이거 잘 구현하면 된대

39페이지

import random

y가 d(정답)

SLP_SGD 맞게 구현하래

40페이지

에러가 줄었다가 늘었다가 하기도함

오른쪽은 w값

924번째까지가 에러가 제일 작네
이쯤에서 멈추면 좋앗겠지
그니까 오른쪽 w의 결과는 924에서 멈춘것보단 안좋은 w

41페이지

Test라 써있지만 사실 iris 데이터를 모두 넣어 테스트하는것이므로 트레이닝이라고 볼 수 있음

w에다가 x를 하나하나넣는거래<?>

argmax 줄 어느 클래스인지 담김

target 정답

w가 랜덤이니까 실행마다 값이 다르게 나올 수 있음
random_state를 주면 편리

42페이지

중요

w세줄
w를 만들어가는 과정

for문 돌리는 이유
한행씩 읽어서 w에 넣고 정답 나오면 w업데이트하고 이런식이니까

안의 for문 행 쭉 다도는거
바깥 for문 w를 1000번 업데이트니까 1000번

회색박스
안에 행렬 전치 어떻게 시킬지 알아봐야함

## DC_chap9_5

45페이지

에러를 어떻게 측정하는가? -> cost func = loss func
에 대한 얘기

단순하게 d-y말고 더 좋은 방법을 설명하실거래

46페이지

m: 아웃풋 노드의 개수

아웃풋이 여러개면 그걸 다 더해서

y1 d1
y2 d2
y3 d3

제곱하기 때문에 w가 음수인쪽은 그래프가 그려지지 않고 모양이 이런 것

48페이지

크로스 엔트로피

d는 0아니면 1인데

에러가 빠르게 줄어듦(46페이지보다)

50페이지

weight matrix 업데이트

51페이지

미니배치 방식이 제일 많이 사용됨

52페이지

한 행 한 행 입력 넣어 바로 가중치 업데이트하는 방식 -> 이걸 반복하고
iris, 150개 행을 할 때마다 업데이트하니 150회 겠지

53페이지

배치(batch)

첫행 넣고 델타w 구할 수 있음 -> 보관하고 있기
두번째 행 델타w 구함
아직 w 업데이트 안함
150개의 델타w 구하고 이거의 평균 구해서 업데이트는 = 1번

54페이지

미니 배치(mini batch)

데이터를 일정 개수로 잘라서,,,
블럭이 진행되는 동안은 델타w만 구하고(3개)(업데이트는 안하고 ) 델타w 평균으로 업데이트 한번 하고

다음 블록 똑같이 하고

업데이트 횟수 = 블럭의 횟수

미니배치
배치보단 업데이트 횟수 많고
경사 방식보단 적겠지

55페이지

에러가 제일 많이 줄어들게 하는게 미니배치임을 알 수 있음

56페이지

epoch
에폭
1번데이터부터 150번 데이터까지 한번 쭉 학습하면 이게 1epoch임

10회 학습 -> 전체 데이터를 10회 훑었다.

# 201115

- 수업 개요

back propagation 델타룰을 일반화 한것

## DC_chap10_1

3페이지

선형분리만 가능(한계o)

4페이지

multi layer가 해결책

노드 층이 여러개이면 weight 업데이트를 어떻게 하나?
e1? e2?
각 레이어 중간에 선에 있는 에러는 어떻게 알아내나?
-> back propagation으로 해결

5페이지

데이터 인풋값은 오른쪽으로 전달

에러를 역방향으로 전달하겠다.

예측값과 정답으로 에러를 계산을 하겠지?
이 에러가 나오면
마지막 가중치값이랑(왼쪽) 똑같이 연산을 하면 왼쪽 노드의 아웃풋으로서 에러가 나옴

또 이 에러를 왼쪽 가중치랑 연산(인풋이랑 가중치로 연산할 때처럼)

6페이지

각 노드 동글뱅이에 acti func 붙어있다고 생각하기

hidden layer 1,2에는 sigmoid 적용

마지막에는 softmax 적용

hidden layer가 2개인 다층 신경망

(v=가중합)

7페이지

피드 포워드
인풋 x가 최종 아웃풋 y로 전달되어가는 과정

이 y1이 다음 레이어를 위한 아웃풋이 되는거지!

9페이지

마지막단계에서
마지막 가중합 v0
마지막 아웃풋 y0가 나옴

10페이지

에러를 거꾸로 전파시켜가는 과정을 볼 것

델타룰과 같은 내용

갱신 먼저 -> 에러계산

갱신
에러에 액티 펑션의 도함수를 곱해 델타 계산하고
w업데이트할 값 구해서
원래 있던 w랑 더해서
갱신된 w 만들어냄

(델타룰과 똑같음)

2번째 히든레이어의 계산해야함

11페이지

히든 레이어 2번째의 에러를 e2라고 하면

델타0 계산된거랑
W0랑 계산
~ 그리고 과정 반복

백 프로퍼게이션은 가중치들을 업데이트 하는 것이기 때문에
12페이지까지 에러 계산하면 끝!

13페이지

희미한 정도가 에러의 크기

앞쪽에 있는 가중치들이 갱신이 안되는 것
-> 기울기 소실의 문제

원인은?
12페이지보면
델타값을 원래 에러에 미분함수를 곱하잖아
근데 이 미분 값이 점점 줄어들기 때문에
이게 기울기니까 이게 점점 줄아드니까
기울기소실

3,4층 괜찮은데
10층 20층이면 뉴럴 네트워크가 정확하지 않을 수 있음
이거 어떻게 해결? acti func를 어떻게 하느냐에 답이 있음 -> 다음 시간에 살펴볼 것

## DC_chap10_2

15페이지

다층 신경망의 기울기 소실 문제를 해결하기 위한 얘기할 것

기울기 소실 일어나지 않는 activation func중 많이 쓰는게
ReLU(렐루)

에러가 더 빠르게 0에 수렴한대

16페이지

fx는 0과 x값 중 큰 값 취함
x가 -면 0이 도출되겠지
양수이면 입력값을 그대로 출력으로 내보내는(기울기 소실의 문제가 해소될 수 있음)

sigmoid는 0~1사이로 어떤입력이 들어와도 집어넣어버림

얜 쫙 올라가니까 sigmoid의 문제 해결 가능

---

Leaky ReLU

x가 음수가 나오면 계속 0으로 남아 학습이 안되는 문제를 해결
0보다 작으면 0.01이런값을 곱해서 아예 없어지진 않도록 하는

17페이지

ReLU함수 단순

x로 입력줘서 해보기, 함수 단순히 쓰면 됨

18페이지

모멘텀 Momentum

back propagation에 많이 쓰이는 activation func인
모멘텀!

학습이 일어날 수록 에러가 일반적으론 줄어들음
w값에 따라 선형적으로 줄어드는게 아니라
진동함
줄다가 늘다가 할 수 있음
만약 a지점에서 에러 다시 증가해서 학습 멈추면 에러가 큰 상태에서 학습 중단하는 거기 때문에(로컬 미니마 문제) 문제있음
여기에 빠지면 에러를 더 줄일 수 없음

모멘텀은 이 문제를 해결, 방지해줄 수 잇는 기능 중 하나.
어떻게? 진동 진폭을 덜하게할 수 있음

19페이지

모멘텀의 목적
학습시간을 빠르게 한다
W값이 진동하는(갱신시 값이 변하잖아) 폭을 줄이는(변동폭 줄이는)

어떻게 줄이냐?
W의 변동폭을 조절 가능

20페이지

모멘텀 달기 전에 진동폭은 큰걸 볼 수 있음

모멘텀 사용하면
진동 횟수, 폭 줄어듬
그래서 빨리 목표에 도달

21페이지

원래 w 갱신하는 방식

모멘텀은 여기에 모멘텀을 달음

22페이지

모멘텀은 어떻게 적용?

m짝대기는 이전 모멘텀임

모멘텀도 갱신이 됨

w를 직접 더하는게 아니라 모멘텀을 더해 w를 갱신

m안에는 이전 스텝의 w값을 포함하고 있음

w가 함부로 움직이지 못하게함

델타 w에 어떤 값을 더해서 그걸 가중치에 더하니까 w값이 더 커질거고 변화되는 값이 커져
진동은 덜하는거지

학습속도가 그래서 빠른 것

## DC_chap10_3

25페이지

인공신경망의 과적합 문제(빈번하게 나타남)

과적합: 주어진 데이터만 학습하다보니 그거랑 조금만 다르면 예측을 잘 못해내는 문제

drop out이 과적합을 해결하는 좋은 방법

weight decay 방법도 있는데 그거보다 드롭아웃이 좋대

드롭 아웃은...
x표시된 노드들을 끊어버리는 것(노드를 없는 걸로 침)
이 노드와 연결된 가중치는 없는 것처럼 됨
가중치 선을 끊는 것과 같은 효과
노드 몇개를 드롭아웃하는것

효과는?
학습을 방해하는 것
왼쪽 그림의 fully connected보다
노드를 랜덤하게 빼버리면
연결된 애들만 갱신이 일어나니까
학습이 더 된다(연결된 애들만 갱신이 일어나니까)
학습 시간 길어짐
그렇지만 좋은 점은 과적합을 막을 수 있다.

26페이지

weight decay
가중치 감소

가중치를 감소할 때 데이터에 딱 맞게 조절하면 과적합이 발생할 수 있음

아래 식에서
0과 1사이의 값을 곱해 갱신하면
원래 갱신해야될 값과 다른 값으로 갱신되니까(학습이 잘 안되게 방해)
학습시간 길어지는 대신
주어진 데이터를 너무 열심히 학습하는 것을 방지하여 과적합 막을 수 있음

27페이지

드롭아웃은
트레이닝 단계에서 하는 것
개념적으로 끊어졌다는 거지 몇개만 골라서 한다는 것

학습단계에서의 과적합 방지 효과냄
몇퍼센트 끊을 것인지 p로함(확률, 0과1사이값)
p는 몇퍼센트를 남길것인가임. p가 1이면 다살아남앗으니 드롭아웃 하나도 안했다는 것

test에서는
다 사용해서 학습함

28페이지

x축 p
오른쪽으로 갈 수록 살아남는 비율이 높은 것

y축 에러가 얼마나 줄어드는지

p값이 1이면
학습할 때 에러는 0인데
실제 테스트 할 때는 에러가 높네?
이게 과적합의 예시임
드롭아웃을 안하면 이처럼 과적합이 나타날 수 있음

p값이 0에 가까우면 둘다 에러 높음(언더피팅)

50프로 정도 될 대
테스트 에러가 많이 줄었음

29페이지

데이터 사이즈

데이터 수가 많으면 에러율이 적고
두 선 사이 차이가 별로 없음
드롭아웃의 효과 미미
데이터 값 많으면 과적합도 많이 안 일어남

데이터 수가 작으면
드롭아웃을 한 경우가 에러가 더 높음
드롭아웃 안하는게 낫다

데이터 수가 10000개 못미치는 정도에 하는게(7천개 8천개 정도)
드롭아웃 효과가 높다는 것

30페이지

마지막 주제
w값을 초기화 하는 것

weight값에 초기값을 줘야함
아무값이나 주면 안되나?
학습이 안 일어날 수도있어 좋지 않은 방법임

아래 두개는 초기화하는 방법

31페이지

중요하지 않음 (파이썬에서 함수로 다 제공이 되기 때문에)

인풋의 개수 sqrt 루트 씌운 것을 사용해 초기화

가중치 w의 값을 초기화 할땐 굉장히 작은 값을 주는게 좋대(상식적)

32페이지

복잡하게 이런 식을 외우거나 할 필요는 없대

33페이지

가중치 초기화하는건 액팁펑션과도 연관성 높음

relu 쓰면
he 초기화 방법이 좋다(학습이 잘된다던가 정확도 높아진다던가)

34페이지

이번 수업 키워드 정리

다층신경망에서 학습이 어떻게 일어나는지
백프로퍼 문제
위를 해결하기 위한 펑션 relu
모멘텀
과적합 방지가능한 드롭 아웃
학습을 잘 일어나게할수잇는 가중치 초기화 방법

=> 딥 뉴럴 네트워크에 대한 이론적인 부분들을 살펴봤고 다음시간부턴 실전

# 201117

- 개요

(참고링크)

아나콘다 가상환경 관련
https://niceman.tistory.com/85
https://like-edp.tistory.com/entry/Anaconda%EC%95%84%EB%82%98%EC%BD%98%EB%8B%A4-%EC%97%90%EC%84%9C-Keras%EC%BC%80%EB%9D%BC%EC%8A%A4-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0

import tensorflow as tf 에러문
https://junho0956.tistory.com/273

가상환경에서 spyder 설치
https://eehoeskrap.tistory.com/401

(나)anaconda 가상환경 myvenv 이름으로 생성

```shell
아나콘다 프롬프트에서

# 아나콘다 가상환경 생성
conda create --name myvenv python=3.6

# 생성된 가상환경 실행
conda activate myvenv

# (설치된 가상환경 실행 후 ) keras 설치
conda install keras

# 설치된 모듈 확인
conda list

# 가상환경에서 spyder 설치
conda install spyder

# spyder 실행
spyder

# 가상환경에 없는 패키지 설치
conda install scikit-learn
conda install pandas
conda install matplotlib
conda install numpy


```

## DC_chap11_1

3페이지

다른 프레임워크에 대한 인터페이스를 제공하는 형태

keras를 통해 tensorflow를 이용하는 방식으로 사용할 것

5페이지

모듈을 조합하여 모델을 구성

6페이지

두 사이트에서 필요한 keras 함수 예제 나와있으니 이용하기

8페이지

7페이지에서 설치 후
잘 설치됐는지 확인하는 과정

9페이지

2개의 히든 레이어를 갖는 neural network

iris 데이터셋은 품종이 3개이기 때문에 맨오른쪽 숫자 3인 것

히든레이어 함수는 relu, 아웃풋 함수는 softmax

11페이지

Sequential
모델 종류 중 하나. 아웃풋으로 쭉 흘러가는 피드포워드(feed forward) 형태를 시퀀셜이라함. 일반적이기 때문에 이거 사용할 것

Dense
레이어를 구현하는데 쓰임

나머지는 부수적인 것들

12페이지

X 앞의 4개의 컬럼

Y 품종
-> 문자열 형태로 되어있기 때문에 숫자로 바꿔야함 -> LabelEncoder() 이용
neural network에서는 이걸 원핫코딩
0은 100, 1은 010, 2는 001(3개의 비트로 표현한걸로 바꿔줌)
(dummy_y = ...)

14페이지

epoch 전체 데이터를 쭉 훑어서 학습을 시키는 것
이걸 50번 하겠다는 것

batch_size 하나의 에폭안에서 데이터를 10개씩 끊어서 10개 지나가면 업데이트하고 10개지나가면 업데이트한다는것!

만드려는 모델이 시퀀셜 모델이다(인풋에서 주어진 값이 쭉 흘러가면서 아웃풋에 도달하는 형태)

Dense = 레이어
input은 첫번째 히든레이어와 묶어서 표현하게됨
그래서 레이어가 3개인 것!
(input+첫번째 히든레이어 / 두번째 히든 레이어 / 아웃풋)

dense 첫번째 줄 코드
노드의 수 10개 / input 디멘젼이 4다 / 가중합 계산 후 acti func는 relu함수를 이용하겠다

dense 두번째 줄 코드
노드수 10개

dense 세번째 줄 코드
아웃풋 노드의 개수에 맞게 3개로 설정되어있음

여기가지 하면 모델이 정의되는 단계가 끝났음

model.summary() 모델의 스트럭쳐 보여줌

15페이지

output shape
10개 나가고 10개 나가고 3개 나가고

모델에서 갱신되는 값 몇개인지 = param #

첫 레이어에서
연결선이 input에서 히든레이어로 4\*10 개 있음 = 40
가중합 v는 노드수만큼 있어 10개이므로
40+10 = 50

10\*10 = 100

- 10
  => 110개

param # 이게 학습되면서 계속 변동한대

다시 14페이지

모델 컴파일

loss => 에러 계산 함수의 종류를 지정
optimizer => 학습할 때 속성값들을 잘 컨트롤해서 에러를 줄어들게 하기 위해 모델이 포함하고 있는 여러가지를 조절하는데... 이때 조절하는 알고리즘을 optimizer라고 함(adam이라고 많이 쓰는 알고리즘 씀)
metrics => 성능 평가를 뭘가지고 할거냐! 분류에서는 accuracy 많이 쓰지!

16페이지

모델 선언 -> 모델 컴파일 -> 모델 피팅

모델 피팅
실제 학습이 일어나게 하는 과정

verbose 학습 실행 중 여러 메시지를 어느 정도로 보여줄 건지(숫자가 높을 수록 자세히 보여줌)

epoch거치며 weight값이 바뀌자녀
이걸 중간중간 평가하고 싶을 때
validation 사용(이 때 데이터를 뭘 쓸까 이런거임) -> 학습에 영향을 미치진 않음(단지 검토용)

결과화면
train 90개의 행이 있다
validation에는 60개의 행이 있다(test)

epoch 1/50
50개중에서 첫번째 에폭을 수행하고 있다
시간 / 에러값 loss(예측값 y와 정답 d 사이의 에러 계산했을 때 총합이)
accuracy(트레이닝 acc)

val_loss(테스트 데이터 이용해 에러 계산한 것)
val_accuracy(테스트 acc)

정상적이라면 학습 될 때마다 에러는 줄어들고 정확도는 올라가야함

아무리 가도 epoch 진행될 때마다 에러가 줄지 않고 정확도가 올라가지 않으면 모델을 수정해야할 것!

17페이지

model 안에 weight matrix, acti func 다 들어있는거지
이제 예측

예측 결과 보면
첫번째껀
0번 품종 / 1번 품종 / 2번품종
값중 0번이 제일 크니까 0번일 것이다.

y_class -> 맥시멈값이 어느 컬럼에 있는지 구해서
결과에 오른쪽 처럼 클래스를 간단하게 출력해줌

예측값 나왔으니
원래 모델이랑 비교해야겠지
score => test값을 줬으니 test accuracy 담겨있을 것
score[0]은 loss값 , [1]은 accuracy

정확도가 근데 생각보다 높지 않네,,,
0.73
여러가지 이유가 있을 것
-> 히든 레이어, 노드 수 늘릴 수도 있음(해결하기 위해서)

딥러닝이라고 해서 다른 알고리즘보다 다 좋다라고 말할 수 있는 건 아님

18페이지

plot 으로 그림 그리는 것
accuracy가 떨어질 수도 있음!
진동을 하지만 전체적으론 증가하고 있다는 것

트레이닝acc는 좋아지는데
테스트 acc는 진동하며 제자리 걸음이면
과적합이라고 볼 수 있음

19페이지

weight 값 초기화가 랜덤하게 이루어지기 때문에
그래프는 개인마다 다를 수 있음
random_state 시드값을 셋팅해서 학습과정을 다시 재현할 필요가 있음-> 나중에 다시 볼 것

20페이지

weight값은 어떻게 생겼나?
layer값을 출력하는 코드!

layer 수만큼 돌아가며 출력하는 것
layer 숫자(32,33)은 크게 신경쓰지 말 것

행의 수가 4, 열이 10개
4\*10 의 첫 레이어 보여주고 있음

21페이지

모델을 어딘가에 저장해놓고 불러다 쓸 수 있음
model.save(경로명+모델이름(location))

22페이지

keras 제공 함수
initializers 지정 가능

23페이지

act func도 많음

sigmoid 기울기 소실의 문제(레이어 많아지면)

24페이지

에러를 어떻게 계산하나?

mean squ 리그레션일때
categorical 범주 맞히는 문제일 때

25페이지

optimizer
loss를 줄여가는게 우리의 목표
그 전략을 담고있는게 optimizer
관련된 속성을 체인지하는데 무슨 알고리즘을 쓸 것이냐

26페이지

모델 평가 함수
acc 많이 씀

지금까지 keras를 이용해 다층신경망
구성, 학습, 모델 평가 과정을 학습했음

## DC_chap11_2

30분짜리

3페이지

컴퓨터는 숫자 식별 불가해! 식별할 수 있게 해줘야함
필기체 인식 -> classification 문제로 푼다

4페이지

28\*28(픽셀)

2차원 형태 데이터는 학습 불가
우리가 살펴본 네트워크의 구조상 input 데이터가 다 1차원으로 되어있어서... 2차원은 우리가 배운 적 없기 때문에!

픽셀 주루룩 한줄을 일렬로 다 이은 것임. 그래서 784개

neural network는 숫자를 0~1사이의 값으로 변환해서 많이 사용함

입력사이즈가 784개이고
아웃풋 노드의 개수가 10개인
네트워크 설계해야함

5페이지

mnist 데이터셋을 가져옴

빨간색 -> 모델 만드는데 관련된 모듈들

Flatten 2차원 형태의 데이터를 1차원으로 평평하게 만들 때 쓰는 모듈
Dropout 과적합 방지. 노드의 일부를 끊는 기능

6페이지

mnist 는 train, test / 데이터와 label이 다 나눠져있대
받을 때 이렇게 받으면 됨

픽셀의 크기를 0~1사이로 tranform하기 위해서 255.0으로 나눔

- one hot encoding

x_train
28\*28
60000장의 이미지가 있다는 것

초록색 -> 안에 있는 값들

7페이지

설계하려는 네트워크 구조는 이럼

28\*28
2차원 배열을
일렬로 펴주는게 flatten
flatten에 집어넣으면 784개로 일렬로 펴준대
이걸 인풋으로 넣는대(첫번째 히든레이어에)
-> 256개의 노드에
사이에 weight가 있겠지
가중합은 relu를 통해 액티펑션 거치는데
그 결과를 drop out을 일정비율만큼 줘서 거쳐 나가게 함
그리고 그 다음 레이어로 주는

drop out에서 살아남은 애들이랑
그 다음 히든 레이어랑 연결이 되는 것(128개)

인풋, 아웃풋은 사이즈가 정해져있음
결과는 softmax 이용 -> 확률이 나올 것 -> 가장 높은 확률값가진 것이 예측한 클래스가 된다

8페이지

keras로 구현해볼것

input shape이 28\*28인걸 쭉 펴주는 flatten

노드의 개수가 256개, acti func 는 relu

drop out rate 0.4, 0.5많이 씀(살아남는 노드의 비율을 주는 것)

9페이지

shape은 노드 개수

drop out 노드 개수는 구조상 사이즈가 256개라는거고
256개 중에 몇개가 죽겠지

10페이지

모델 컴파일 과정
모델 학습 시키는 과정

맨 첫줄에 파라미터 여러개 줄 수 있는데 가장 많이 쓰는 것이 learning_rate

validation split 모델 만들 때 train_X 데이터 중 일부를 떼어서 (20프로 정도) validation용도로 쓰겠다.(training이 어떻게 되고 있는지 검증하ㅣ기 위해서)
-> 실제로 학습은 train_X의 80퍼만 가지고 하겠지
이걸 안하고 싶으면 앞에서 본것처럼 test데이터를 validation 데이터로 줄 수도 있음 but 지금 배우는게 일반적인 형태임

11페이지

loss는 줄고 acc는 늘어야 정상

하나의 에폭 안에서
정해준 batch사이즈에 의해
일정한 인스턴스 개수를 끊어서 한번에 w값을 갱신해가자녀
그 가중치가 변경될때마다 loss, acc 값들이 변경되는것을 눈으로 확인할 수 있을 것(실행시)

12페이지

print(pred) (one hot encoding 된 결과로 출력됨)

print(y_classes) label값으로 출력됨(이 코드 윗라인에서 변환을 했기 때문에)

model.evaluate 모델 평가
test_x값에 의한 예측값과 test_y를 비교해 loss, acc를 score에 담게됨

모델 피팅시 중간중간 계산 결과가 앞선 disp에 저장됐을 건데
이를 이용 그래프를 그릴 수 있음

13페이지

test acc 굉장히 높게 나오는 것 볼 수 있음

train acc 보면 epoch 늘리면 정확도 더 올라갈까싶지만
validation acc는 epoch 늘어나도 변동이 잘 없자녀 -> 20 넘어가도 정확도 변동은 많이 없겠다 예상 가능

14페이지

loss 볼건디

train loss -> epoch 늘수록 급격히 줄어드는 것 볼 수 있음
loss 가 0인게 좋은것 (에러가 없는것)

epoch 17 넘어가는 쪽 보면 거의 변동이 없지
이 부분이 모델 만들었을 때 취할 수 있는 최고의 acc 라고 볼 수 있음

epoch 어느 시점에서 변동이 평평하면 더 트레이닝해도 acc 늘지 않을거라고 판단하기! 에러 줄어들면 epoch 더 늘려보기도 하고

15페이지

앞서 본 그래프들은
학습 다 끝나고 disp 를 그린것임

실시간으로 학습 상황을 보는 법을 알려주시겠대

trainplot 사용
trainplot.py 파일 이용하면(그래프를 그리기 위한 기능이 담겨있는 파이썬 파일이래)

fitting시 빨간글씨 callback 지정하면
콜백 -> 모델이 피팅할 때 일정시간마다 콜백함수 호출한대
그시점에서 나온 값으로 그래프 그려준대

선이 변화되는 것을 실시간으로 볼 수 있대

loss그래프 보고싶으면 train_plot.py 파일을 좀 수정하면 된대

# 201124

## DC_chap13_1

합성곱 신경망
이미지 분류(ex. 남녀분류, 이미지안에 자동차 유무, 의학분야-CT보고 암진단한다던지)

3페이지

CNN 쓰는 이유(이미지 분류에 우수)

빨간 박스 뒤의 부분이 DNN이 위치한다고 보면되고
convolution layers가 추가됨

4페이지

CNN - 영상처리와 밀접한 관련있음

한칸이 픽셀.
픽셀이 모여 이미지를 형성
동일한 사이즈의 이미지에 몇개의 픽셀이 있느냐 -> 해상도(해상도가 높을수록 선명)

5페이지

이미지는 3차원 배열 형태로 저장됨
색을 나타내기 위해선 R,G,B를 각각 가지고 있어야하기 때문에 각각 채널을 2차원배열로 구성되고
이 3가지가 모여 표현이 되겠지

이미지 자체는 2차원인데 컬러라고 하는 차원이 들어가서 3차원이 되는 것

흑백이미지는 어떨까?
2차원 배열 하나로 다 표현이 됨

픽셀의 크기 나타낼 때
0~1
혹은
0~255
로 표현하는 방법 있음
(보통 영상 처리할 땐 0~1로 바꿔서 처리한대)

7페이지

skimage 모듈 필요

코드에 빨간색 표시 -> 이미지 처리와 관련된 코드

askopen..
파일을 읽어오는데 경로지정하지 않고 탐색기에서 선택할 수 있게하는 기능이래

경로를 포함한 파일이름이 fname에 저장되고
그걸 읽어서 image에 저장
이미지를 보고 싶으면 imshow

8페이지

세로가 1600픽셀이 넘고
가로가 3000픽셀 정도되는 이미지인 것을 볼 수 있음

9페이지

이미지도 배열임을 알 수 있음

배열이니까 shape 보면
1688개 픽셀 세로(가로인듯)
3008개 픽셀
가진 이미지고
3은 채널 수임(R,G,B) (흑백은 세번째 인자 안오고 그냥 가로 세로 픽셀만 shape 결과로 나타날 것)

print(image[:,:,1])
0 R
1 G
2 B
임!!!
여기선 slicing 인자에 1이라 했으니 0인덱스인 red channel만 결과에 나오겠지
결과는 0~255값을 표현하고 있는 배열

10페이지

컬러이미지를 흑백으로 바꾸고 싶다면!
color를 임포트해서
color.rgb2gray 사용

채널이 다 합쳐져 채널이 없는 이미지가 된 것을 볼 수 있음

11페이지

이미지 크기를 변경하고 싶은 경우
transfor 임포트

new_shape 가로 세로 크기를 나누기 2한 것(//2)(채널은 그대로 냅두고)

결과 보면 절반으로 줄어든 것을 볼 수 있음

12페이지

rotate 임포트

flip
좌우 반전

rotate
회전
angle로 회전할 각도 주면됨

13페이지

filters 많이 쓰인대
(뒤에서 더 자세히 볼거긴 하지만..)

sobel_h
물체의 윤곽선을 찾아내는 것

14페이지

처리된 이미지를 새로 저장하고 싶을 때
이미지 명과 저장할 이미지를 인자로 주면됨

아래 링크에 팁이 있으니까 영상처리에 관심있으면 살펴보기

## DC_chap13_2

16페이지

CNN 의 C가 convolution이래

원본에 있던 값에 필터에 있던 값을 곱함
9개 값이 나오면 이걸 다 더한대

필터를 적용하면 새로운 이미지가 나옴(변형되거나 색깔이 바뀌거나 함)

17페이지

필터블록이 옮겨가면서 오른쪽 분홍표 채우는 것 보여주심

18페이지

엠보스(경계부분이강조된, 볼록튀어나와보이는)을 찾아내는 필터를 적용하면 이런 결과 나온대

16페이지

파란 필터의 값들을 어떻게 정해주냐에 따라서 필터의 기능이 달라지는 것!

19페이지

cnn 이미지 자체를(2차원) 인풋으로 받아들임(픽셀들의 위치정보를 분류에 다 활용 가능)
dnn 하면 위치정보 다 날라감(1열로 이으니까)

필터의 내용은 학습이 진행되면서 갱신됨
필터의 사이즈를 몇으로 할지만 우리가 정해주면됨

feature를 여러개 생성가능
필터 여러개면 여러개이미지 나오겠지 이렇게 여러 피쳐를 만들 수 있음

이미지가 가지고 있는 특징
물체가 가지고 있는 특징을 학습이 끝났을 때 최종적으로 가지게됨
이 특징을 이용해 분류하는 것이 cnn

겹겹이 쌓인게 feature
필터가 10개면 피쳐도 10개
피쳐를 만드는 과정 -> convolution

pooling
이미지가 큰데 이걸 줄이는 과정

convolution
또 필터를 적용

pooling

convolution과 pooling이 세트인데
이게 여러 층으로 존재한다는 것
-> 이걸 다 합쳐 convolution layer라고함

이미지니까 2차원이고
이게 여러개 있는데
이걸 일자로 핌(그래야 dnn의 인풋으로 줄 수 있으니)
거기서부턴 우리가 배운 dnn에 의해 분류가 일어나겠지

20페이지

맨 왼쪽 사각형에 있는 블럭 하나하나가
19페이지에서 봤던 feature 한판떼기라고 보면됨

자동차에 대한 학습 다 끝나고
feature에 어떤 이미지가 저장됐는지 본 것임
중간에 있는 사각형은 29페이지의 2번째 레이어에 feature라고 보면됨

총 3레이어

탑 레벨로 올라갈 수록

로우레벨에서는 차의 선이라던가 아주 기본적인 형태를 잡아냄

중간 레벨
추상적이지만 복잡해진 자동차가 가지는 특징들을 잡아냄

탑레벨
조금 더 구체적인 이미지들이 나타남
자동차가 가진 특징들 찾아냄

21페이지

이미지가 3차원이면
각 채널을 분리해
각각 필터를 적용하고
그다음 합침

컬러이건 흑백이건
최종적인 feature map은 동일

22페이지

초록색이 원본이미지
노란색이 필터

몇 칸씩 옮겨가는지가 stride임

stride 2면 세로방향으로도 2칸이겠지

필요에 따라 stride를 조절할 수 있다

23페이지

padding

stride를 1로 하면
`5*5 -> 3*3` 으로 줄어듦

경우에 따라서는 원본이미지의 크기를 유지(보존)할 필요가 생길 수 있음 -> padding
원본 자체를 padding을 채워넣어 크게 한 다음에
필터를 적용

padding이 회색 박스들(테두리에 씌운것)이라고 보면됨

보통의 경우에는 패딩 필요없대

24페이지

pooling

이미지의 크기를 줄이는 용도

`2*2` 사이즈로 구획을 나눠서
각 구획마다
max pooling 이면 최댓값을 대표하는 값으로 뽑는다던지 하는

이미지를 그냥 50프로 줄이는거하고(보통 평균내는거 사용)
풀링으로 50프로로 줄이는거하고 다르다

## DC_chap13_3

26페이지

이제는 cnn을 keras 코드로 표현하는 것을 배워볼 것

feature개수 = 적용할 필터의 개수

pooling이 `2*2`니까 1/2로 줄겠지

`116*156` -> `58*78`

다음 레이어에선 필터를 16개 적용해서 픽셀이 좀 줄어들었음, 피쳐가 16개

마지막에 이 이미지를
flatten 이용 일자로 펴서 dnn
fc(fully connected)

아웃풋이 4니까
이 이미지를 4개의 클래스로 분류하려고 하는구나를 알 수 있음

이런 아키텍쳐를 코드로 어떻게 나타낼까?

27페이지

공식을 외울 필요는 없고

stride, padding 없는 경우는
필터의 -1 만큼 줄음
5면 4만큼 주는거지

근데 stride, padding 들어가면
계산 복잡해지니까 이런 식을 이용하면 알 수 있다~ 는 뜻

28페이지

pooling은 간단

`12*17` 짜리가 20개 있는데(피쳐)
이걸 쭉 늘이면 4080개인데
fully connected layer는 512개로 디자인되어있지

26페이지에서 보이던 그림 보면 바로 이어지는 것처럼 보이는데 중간에 한 단계가 더 있음

4080개가
weight matrix 통해
512개로 매칭된다

30페이지

피쳐의 수가 12(피쳐 12개 만들겠다), 커널사이즈는 인풋의 커널(필터?) 사이즈, 채널수(1이면 흑백, 3이면 컬러), 합성곱을 그대로 쓰지 않고 relu 함수에 통과시킨다는 뜻

그다음은 maxpooling

그다음 convolution
한번 정해지면 계산 결과에 의해 shape이 정해지니까 인풋shape이 필요 없음

Flatten()으로 쭉 피고

## DC_chap13_4

33페이지

mnist 데이터셋을 가지고 cnn을 적용해서 필기체 인식을 하는 이미지 분류의 예를 볼 것

이미지 가로 세로가 28, 28
이미지가 0~255로 픽셀 나타내져있는데 이걸 0~1값으로 바꾸려고 255.0으로 나눠줌

34페이지

여기 #reshape 과정 매우 중요⭐
reshape 배열 크기 바꾸는 함수

첫번째줄 코드
X_train.shape[0] -> 이미지의 개수
img_rows, img_cols 이미지의 행과 열의 크기
1 -> 흑백이라서 넣어준 것
=> shape이 4차원이 되어야함(인자가 4개니까?)

y값은 one hot encoding 해주기

random seed
동일한 프로그램은 돌렸을 때 동일한 결과가 나오도록

num_classes
숫자의 개수가 0~9라서? 10(뭔소리지)

35페이지

border_mode='valid'
패딩할 때 하는건데 valid는 패딩 안쓰겠다는것
(same 이면 원본이미지가 유지되도록 적정하게 주겠다는 것)
stride(행방향으로 1칸, 열방향으로 1칸)
input_shape (흑백이니까 1)

mnist 데이터가 간단해서 레이어 convolution-pooling 세트 한가지만 했대

dropout 과적합 방지

마지막
Dense
아웃풋의 개수는 0~9 (10개)니까 num_classes 인자로 넣어줌

36페이지

model 이제 학습해야겠지

validation train을 잘라서 해도 되는데 여기선 일단 test 갖다 썼대

loss, accuracy test데이터에 대해 출력해봄

37페이지

loss값 줄어드는 것 볼 수 있음

38페이지

epoch에 따라 acc 어떻게 변화하는지 그래프로 보여줌
어느 수준되면 acc 증가세가 멈춤을 알 수 있음

만약에 train은 1에 가까워지는데
test(validation) acc은 어느 구간부터 멈추거나 오히려 떨어진다면
그 구간에서 멈춰야됨을 인지
그 구간의 epoch까지만 진행하도록 코드에서 epoch을 변경해야겠지

39페이지

앞에서 한건 이미 로드된 이미지데이터를 쓴거고

이제는 이미지를 직접 로드해야되는 상황을 보자

레이블 정보가 따로 없고
파일명에
`0_블라블라.png`
이렇게 되어있으면 그 이미지는 0 이라는 것

40페이지

백슬래시 2개(\\)는 슬래시 1개(/) 로 대체해도됨

listdir 각각의 파일에 있는 파일이름을 모두 불러옴

41페이지

이 이미지를 3d -> 4d 형태로 바꿔야함

#load train images
X_train = np.zeros(shape=(len(flist_train), 28, 28, 3))
흑백이어도 마지막인자 3으로 줘야함(이유는 이따 설명할것)

shape의 첫 인자는 이미지의 개수
y_train은 train 데이터의 개수만큼 있겠지

(train 데이터를 읽는 과정이었음)

#for문

image_pate 파일 가져오고
image load (imag_path에서 정의된 파일을 가져오는데 사이즈가 `28*28`)
image를 배열로 바꾸는 img_to_array(img) -> 3차원을 반환하기 때문에 흑백도 위의 shape에서 3으로 준 것 / 근데 이 shape이 X_train이랑 안맞아서 expand_dims로 맞춰줌

이미지 array를 X_train이런데 넣어줌
파일명에서 첫글자를 자르는 과정 (flist_train[idx][:1])

#scaling
0~255니까
0~1 범위로 스케일링

test도 똑같이 하면
이미지 형태로 가져와서 데이터를 준비할 수 있을 것
그리고 cnn아키텍쳐에 맞춰 진행하면됨
나머진 mnist말고 다른 예제에 대해 연습을 좀 해보래

Q. X_test.shape[0] 이런 코드 자세히 어떻게 쓰는건지?

# 201201

## DC_chap14_1

transfer learning 요즘 핫한? 딥러닝 기술?

3페이지

전이학습
아주 새로운 개념은 아니고 머신러닝 분야에서 다양하게 적용하고 있는 분야임

축적된 정보를
원래 A라는 문제를 해결하는 방법을 그와 비슷한 B라는 문제를 푸는데 적용할 수 있지 않냐는 것

자율주행 이런것에서
자동차를 인식하는 솔루션 찾았다고 해보자 -> 이걸 트럭 인식하는데 쓸 수도 있지 않느냐는 것

imageNet에는 1000개 정도의 이미지들이 있는데
일상의 물건은 1000개 정도로 다 표현 가능할 것
모델이 나왔다는 건 뉴럴네트워크, 가중치 도출됏다는 것 -> 이걸 가져다 내 문제를 푸는데 써보자!는 것

4페이지

keras는 다양한 딥러닝 모델 제공
이 모델은 이미지넷에서 우수한 성능을 거둔 모델들이겠지
네트워크구조랑 훈련에서 픽스된 가중치들을 수집해서 케라스에서 제공한다는 것. 이걸 가져다가 우리 문제 해결하는데 쓸 수 있겠지(실생활)

또 다른 이미지를 분류할 때 사용하거나, 좀 더 튜닝해서 다른 목적으로 쓰든가 여러가지로 활용 가능

굉장히 유명한 모델들 리스트업

이중 한두가지 써볼것

5페이지

모델 사이즈
정확도
이 모델이 포함하고잇는 파라미터
레이어(층수)

과거에 나온 모델들이 사이즈가 좀 큰 걸 볼수 있음
inception, mobilnet같이 최근에 나온건 경량화되어잇음

efficientnet 요즘 각광받음

7페이지

이 모델은 클래스가 1000개고
내가 원하는건 10개 정도의 클래스 구분이라면
어찌해야하나?
케라스에서 쉽게 처리할 수 있도록 해줌

8페이지

초반에 나온 좀 무거운 모델

vgg16

CNN
뒤쪽에 파랑~ fully connected layer

input은 224x224인 컬러 이미지
output은 1000개 클래스(1000개의 물체에 대한 분류)

9페이지

case1
기존 모델을 그대로 이용해 이미지 분류에 사용하는 경우

vgg16모델을 로드
이미지를 준비
그 이미지를 모델에 넣어 단순히 결과만 확인
(여기선 내가 가진 이미지 사이즈랑 vgg16모델의 인풋 이미지 사이즈가 다르니까 그것만 조절하면됨. 그럼 아웃풋은 1000개 중에서 어느 하나로 나오도록 되겠지)

10페이지

preprocess_input
내가 가진 이미지를 vgg에 맞춰 전처리

decode_pre~
1000개의 이미지인데 1000개중 500번 이러면 알기 어렵고 컵이다 자동차다 이렇게 하면 쉬우니까 그렇게 해주는 것

vgg16
가져온 모델이라하면 2가지가 포함된것임
neural net architecture랑 학습이 된 weight값

모델을 가져옴
사이즈가 커서 시간이 좀 걸림 model=vgg16()

타겟 사이즈를 224,224로 맞춰줌
why? vgg16모델의 인풋사이즈가 244 244이니까 맞춰주려고
그럼 여기에 우리가 예측원하는 이미지 불러와지고

이미지->배열 형태로 바꿔줌

reshape
지금 예측하고싶은 이미지가 1개이니까
(vgg도 그렇고 다른 모델도 하나의 이미지를 예측해라 보다는 1000개 같이 대용량으로 한꺼번에 주는 경우가 많은데 지금경우는 아니니까)

1 예측하고픈 이미지가 1개라는 것. 만약 10개 예측하고 싶으면 이미지 10개 불러다가 배열에 넣고 여기를 10으로 바꿔주면 되겠지
image[0], [1] 이미지 크기
image[2] 이미지 채널값(컬러인지)

11페이지

vgg에 맞게 전처리

오른쪽에 결과
이미지가 이런 차원임
이미지가 1개이고 가로 224, 세로 224의 컬러이미지라는 것(3)

predict 예측하고 싶은 이미지를 인자로 넣어줌

모델을 디자인하고
컴파일하고
fitting하는 과정이 생략되어있음
왜냐면 이미 그렇게 한걸 가져와서 하는거기 때문에 그런 작업이 필요없는 것!

pred 출력해보면
이게 확률값임
1000개가 나옴
1000개의 분류가 있을건데 각각에 대한 확률값이 나오는 것임. 이중 확률값이 제일 높은게 output이 되겠지.

이렇게 예측을 하면 바로 이 이미지가 뭐다라고 나오는게 아니라
1000개의 분류에 대한 확률값이 나옴
제일 높은 확률을 찾아내는 걸 이제 해야겠지

12페이지

디코딩거쳐
label 알아내는 것
여기 담기는 정보는 1000개 중 확률값이 높은 사항 5개의 정보가 담김
클래스에 대한 id, 이클래스는 커피머그다 라는 것, 커피머그일 확률이 72퍼센트 정도라는 것

label[0][0] 출력하면 가장 높은 확률인 첫줄이 담김

[0]은 출력할 필요 없고 [1](coffee_mug), [2]->확률이니까 100곱해줌

13페이지

로딩되는 vgg 모델은 아웃풋 1000개인거 마지막 보면 알 수 있겠지

14페이지

case 2

cifar-10이미지들을 분류하고 싶은데
이미지넷에서 만들어져있는 모델 가져다가 이걸 수정해서 내 문제에 맞추는 응용

cifar10
클래스가 10개임
한 클래스당 6000개의 데이터 있고
클래스가 1000개이니
60000개의 데이터가 있겠지

15페이지

case1과 다르게
내 모델을 만드는데 vgg모델을 수정해서 쓰기 위해 파라미터줌

imagenet에서 훈련으로 나온 가중치 쓰겟다는 것(다른것도잇긴잇대) -> 구조와 가중치 다 가져옴
include_top vgg모델에서 탑부분은 안쓰고 내가 만들겠다는 것
내가 가진 input 모양은 이거다~(cifar 이미지 의 크기)
그럼top은 어디냐?
13페이지에서 보면 flatten-fc1-fc2가 fully connected layer인데 여길 모델에 있는거 안쓰고 내가 만들겠다는 거(false니까)
이렇게하면 모델을 수정하기 때문에 컴파일, 학습 과정 거쳐야함
epoch은 2번만(시간 많이 걸려서. 우린 늘려서 하기)

16페이지

base model

여기에 내 나름의 레이어 쌓음

17페이지

base layer에서 가장 마지막 레이어를 가져옴

이제 레이어 추가해야함

last에 flatten추가해 x에 저장
x에 dense추가해 x에 저장

마지막 레이어 붙여주는 과정 output

base와 output 연결하여 새로운 모델 만듦

18페이지

빨간박습분은 우리가 만든거라
아직 학습전
가중치임

19페이지

컴파일
학습
기존에 cnn에서 배운 내용

이게 기존 모델 고쳐 나의 경우에맞춰 쓰는 방법이었음

## DC_chap14_2

22페이지

vgg는 초기에 나온 모델이라
최근에 나온 efficientNet 공부할 것

b0~b7까지 차이점이 뭐냐면
b7로 갈수록 이미지 사이즈 ⬜ 가 커진대
처리하고싶은 이미지가 있을 때 모델의 인풋이미지 사이즈가 작을 때는 이 이미지를 축소시켜야되서 손실되는게 생기겠지
근데 인풋이미지사이즈가 큰 경우 축소를 좀 덜하거나 손실을 줄일 수 있겠지
그래서 b7로 갈수록 정확도가 높아지는 대신에 큰 이미지를 가지고 모델을 만드니까 가중치가 많아지고 모델의 덩치가 커지겠지

-> 적당한 레벨(b0~b7)을 골라서 할 필요 있음
우린 컴퓨팅 사이즈 작으니 b0으로 해볼 것

23페이지

x축
모델이 가지고 있는 파라미터의 값들
(학습과정에서 변경되는 값들(가중치 포함))

그래프에 유명한 knn 모델들이 있음

b7보면 정확도 높은 것 알 수 있지!
전체 모델의 사이즈는 경량인 것을 볼 수 있음

amoebanet-c 의 경우
정확도는 높은 편인데 대신 파라미터가 많음.-> 무게가 있는 . 사이즈가 큰 모델

b0는
진짜 경량. 대신 성능이 좀 떨어짐

오른쪽 그래프

flops 부동소수점 계산량
파라미터와 비슷한 의미인데
파라미터의 사이즈 크다는 얘기는 계산량이 많다는 얘기니까..

=> efficientnet이 정확도 높고
파라미터는 굉장히 적은 편에 속한다는 것을 그래프를 통해 알 수 있음

24페이지

최초의 input 이미지 resolution이 나옴

b7은 600 x 600 이미지를 첫 인풋으로 한대

우린 b0로 할 건데, 시간 있으면 다른것도 해보기

25페이지

원래 conda install 명령어로 하는데 아직 아나콘다에 efficientnet이 포함이 안된듯
그래서
pip 사용

26페이지

cifar10 데이터 셋 이용

base 모델 만들 때 B0를 이용(시간적 여유 있어 다른거 쓰고 싶다 그러면 여기 숫자 바꾸면 됨)
inclue_top 우리의 데이터 셋(cifar)에 맞춰서 top 부분을 구현할거기 때문에 false로

epoch 좀 늘려도 된대 일단 이렇게

27페이지

base모델 구조 보여줌

28페이지

base 모델의 가장 아래 있던 것 가져옴(top_activation)

flatten
레이어 하나
dropout
붙여줌
마지막 output 레이어도 붙이고

마지막으로 베이스 모델과 우리가 만든 레이어 붙임

30페이지

모델 컴파일
모델 학습(fit)

score 확인

epoch 한번 도는데 535초 걸림
대략 9분 정도 걸림
(vgg16은 같은 작업하는데 20몇분 나왓는데)
loss, acc은 vgg와 거의 같음(시간을 짧게걸리니까 이게 더 효율이 좋은 거겠지!)

32페이지

모델불러올때 쓸 수 있는 파라미터 중 하나가
drop_connect~ 라는게 있는데
원래 이미지넷 자체에 과적합을 방지하기 위한 레이어들이 있는데 추가적으로 과적합을 줄이기 위해 dropout과 유사한걸 해주라는 것

과적합이 너무 발생할 때 추가해보기
(불러오는 모델 자체에 과적합을 줄여줄 수 있으니까!)

34페이지

## keras regression

keras로 neural network 해서 분류문제에 주로 썼는데
regression에도 쓸 수 있다는 것

- 출력이 1개이므로 원핫인코딩 불필요

- boston 데이터 셋 이용하여
  13개의 변수로 주택 가격(medv) 맞추고 싶을 때

35페이지

regression도 sequential 모델임

단순한 neural network인듯

모델 컴파일시 이건 분류가 아니라
loss 를 mean_squared 로 변경(분류문제때와 달리)

36페이지

loss가 많이 줄어드는 것을 볼 수 있음

37페이지

테스트 데이터로 예측하고 결과를 flatten 시킴

예측을 정답결과와 같이 출력하는 부분이 for문

결과의 첫줄 보면
정답이 7.x고 예측값은 9.4
단위가 1000불이니까 2000불 정도 차이나는 것임

레이어를 좀 더 늘리고 epoch도 좀 늘리면
이런 차이가 줄어들 것

39페이지

딥러닝
-> 이미지 분류, 자동 번역등 많은 분야에 쓰이지만 수업에서 다 배우기엔 한계 o

어떤 분야에 딥러닝이 적용될 수 있는가?

1. 이미지 분류

ex. 필기체 글씨 인식, 이미지가 어떤 물체인지 맞추는

40페이지

2. image classification with localization

이미지에 개가 있는데 개가 어디에 있냐 도 알려주는

41페이지

3. object detection

사람도 있고 동물도 있고

물체를 이미지안에서 구별해내는.

사진 안에 동물, 사람, 물건 등 다 다른 object로 구분하는
내가 타겟팅하는 object들이 이미지 내에 있는지. 있다면 어디 있는지 식별해내는

ex. 사진안에 동물이 몇마리 있냐?

여기에 사용되는 모델은 cnn과는 다른 아키텍쳐를 갖는 딥러닝 모델을 사용한대

42페이지

4. object segmentation

물체의 영역을 표시해주는
(물체가 여기 있다! 는 것은 아까 본 detection임)

object들이 세그먼트 되어있음

의학에도 많이 쓰임
연두색이 종양이 있는 부분. 글고 정상적인 부분을 볼 수 있음
어디가 종양인지 사이즈는 얼마인지 등을 이미지로부터 알아낼 수 있겠지

43페이지

5. style transfer

고흐의 사진을 학습시키면
다른 사진을 줬을 때 고흐 스타일의 그림으로 바꿔주는

44페이지

6. image colorization

흑백이미지를 이미 학습된 다른 이미지를 이용해 색깔을 입히는 그런 예

45페이지

7. image reconstruction

이미지의 일부가 지워져있다거나
이미지에 쇠창살? 같은게 있어서 그걸 빼고 채우고싶다던지 그러면
이미학습된 다른 이미지를 이용하여 복원을 할 수 있다는 것

포토샵이 아니라 딥러닝 기술로 한거래

46페이지

8. image super resolution

이미지가 흐린걸 해상도 높일 수 있음

원본이미지는 맨오른쪽인데
그걸 흐리게 만든게 맨 왼쪽

두번째 세번째는 서로 다른 모델을 이용해 원본과 유사하게 해상도를 높인 것
(사진 위의 이름이 모델종류임!)

47페이지

9. image synthesis
   이미지 합성

# 기말과제

## 2차과제(cifar-10 이미지 분류 SW, 전이학습)

파일명이 한글일 경우 파일이 정상적으로 저장되지 않음을 확인했음
('jpg' 이렇게만 저장됨)

Anaconda에서 Flask 환경 설정, Hello world 띄우기
https://m.blog.naver.com/PostView.nhn?blogId=joonb14&logNo=221332333936&proxyReferer=https:%2F%2Fwww.google.com%2F

flask 모듈? 나누기 관련
https://stackoverrun.com/ko/q/11207573

flask html 파일 render할 때 에러난거 해결한 방법
https://exchangeinfo.tistory.com/64

flask 이미지 업로드 및 출력
https://hololo-kumo.tistory.com/143
https://www.python2.net/questions-301187.htm
아래꺼도움됨
https://blog.naver.com/PostView.nhn?blogId=dsz08082&logNo=221855380626&parentCategoryNo=&categoryNo=134&viewDate=&isShowPopularPosts=false&from=postView
https://wings2pc.tistory.com/entry/%EC%9B%B9-%EC%95%B1%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%94%8C%EB%9D%BC%EC%8A%A4%ED%81%ACPython-Flask-static-%ED%8C%8C%EC%9D%BC

https://kkamikoon.tistory.com/155?category=825129

flask file upload 도움됨
https://niceman.tistory.com/150
https://m.blog.naver.com/PostView.nhn?blogId=shino1025&logNo=221361074139&proxyReferer=https:%2F%2Fwww.google.com%2F
https://blog.naver.com/PostView.nhn?blogId=dsz08082&logNo=221868934940&parentCategoryNo=&categoryNo=134&viewDate=&isShowPopularPosts=false&from=postView

https://niceman.tistory.com/151?category=940948
https://my-devblog.tistory.com/24
https://frhyme.github.io/python-libs/file_upload_on_flask/
https://stackoverrun.com/ko/q/12338955

cifal-10 이미지 분류 모델
http://blog.naver.com/PostView.nhn?blogId=qbxlvnf11&logNo=221369724614&parentCategoryNo=&categoryNo=87&viewDate=&isShowPopularPosts=false&from=postView

flask image url 관련
https://stackoverrun.com/ko/q/3220776

## 기말대체과제(흉부 X-ray, 전이학습 사용x)
